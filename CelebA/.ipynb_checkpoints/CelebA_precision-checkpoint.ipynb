{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2c10ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CelebA\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import copy\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import utils as ut\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba6a6e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/mnt/efs/fairclip/FinalCode/CelebA/../utils.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ced5d98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:3\" #if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0e3531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_file = pd.read_csv(\"../../celebA/celeba/list_attr_celeba.txt\", skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b6cac21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000001'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = pd.read_csv(\"../../celebA/celeba/list_eval_partition.txt\", delim_whitespace=True, header=None, index_col=0)\n",
    "splits.index.values[0][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c3dbd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_CelebA(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            \n",
    "            features = model.encode_image(images.to(device))\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features), torch.cat(all_labels).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "399bd275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CelebA_ds = CelebA(\"../../celebA\", split=\"test\", transform=None, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2624dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CelebA(split='test'):\n",
    "    \n",
    "    assert split in [\"train\", \"test\", \"val\", \"all\"], \"split must be either 'train', 'test', 'val' or 'all' for CelebA\"\n",
    "    if split=='val':\n",
    "        split = 'valid' \n",
    "    \n",
    "#     root = os.path.expanduser(\"~/efs-clip-experiments\")\n",
    "    CelebA_ds = CelebA(\"../../celebA\", split=split, transform=preprocess, download=True)\n",
    "    CelebA_features, CelebA_labels = get_features_CelebA(CelebA_ds)\n",
    "    # normalizing features\n",
    "    CelebA_features /= CelebA_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    CelebA_labels = {\n",
    "#                     'arched_eyebrow': CelebA_labels[:,1],\n",
    "#                     'baggy_eyes' : CelebA_labels[:,3], \n",
    "#                     'big_lips': CelebA_labels[:,6], \n",
    "#                     'big_nose': CelebA_labels[:,7],\n",
    "                    'black_hair': CelebA_labels[:,8],\n",
    "                    'blond_hair': CelebA_labels[:,9],\n",
    "                    'brown_hair' : CelebA_labels[:,11],\n",
    "#                     'bushy_eyebrows' : CelebA_labels[:,12],\n",
    "#                     'chubby' : CelebA_labels[:,13],\n",
    "#                     'double_chin' : CelebA_labels[:,14],\n",
    "                    'glasses' : CelebA_labels[:,15],\n",
    "#                     'high_Cheekbones' : CelebA_labels[:,19],\n",
    "                    'gender': CelebA_labels[:,20],\n",
    "                    #'race': 1 * np.logical_and(CelebA_labels[:,6]==1, CelebA_labels[:,8]==1),\n",
    "#                     'oval_face': CelebA_labels[:,25],\n",
    "#                     'pointy_nose': CelebA_labels[:,27], # remove this \n",
    "                    \n",
    "                    'smiling': CelebA_labels[:,31],\n",
    "                    #'straight_hair': CelebA_labels[:,32],  \n",
    "                    'wavy_hair': CelebA_labels[:,33],  \n",
    "        \n",
    "                    'earrings': CelebA_labels[:,34],  \n",
    "                    'hat': CelebA_labels[:,35], \n",
    "                     \n",
    "                    'necktie': CelebA_labels[:,38],\n",
    "#                     'necklace': CelebA_labels[:,37],\n",
    "                     \n",
    "                    }\n",
    "    CelebA_attr_to_int_dict = {'gender': {'female': 0, 'male': 1}, \n",
    "                              # 'race': {'not-black': 0, 'black': 1}\n",
    "                              }\n",
    "    CelebA_int_to_attr_dict = {'gender': {0: 'female', 1: 'male'}, \n",
    "                               #'race': {0: 'not-black', 1: 'black'}\n",
    "                              }\n",
    "    \n",
    "    group_sizes = copy.deepcopy(CelebA_attr_to_int_dict)\n",
    "    for attr in group_sizes.keys():\n",
    "        for group_name, group_val in group_sizes[attr].items():\n",
    "            group_sizes[attr][group_name] = np.sum(CelebA_labels[attr]==group_val)\n",
    "            \n",
    "    CelebA_ = {\n",
    "        'features': CelebA_features,\n",
    "        'labels': CelebA_labels,\n",
    "        'int_to_attr': CelebA_int_to_attr_dict,\n",
    "        'attr_to_int': CelebA_attr_to_int_dict,\n",
    "        'nr_groups_to_consider': {'gender': 2, \n",
    "                                 # 'race': 2,\n",
    "                                 },\n",
    "        'group_sizes': group_sizes \n",
    "    }\n",
    "    return CelebA_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd731ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:01<00:00,  3.27it/s]\n"
     ]
    }
   ],
   "source": [
    "data = get_CelebA(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfe969e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black_hair, 0.27 5422 0.53 0.47\n",
      "blond_hair, 0.13 2660 0.93 0.07\n",
      "brown_hair, 0.18 3587 0.74 0.26\n",
      "glasses, 0.06 1289 0.25 0.75\n",
      "gender, 0.39 7715 0.00 1.00\n",
      "smiling, 0.50 9987 0.69 0.31\n",
      "wavy_hair, 0.36 7267 0.84 0.16\n",
      "earrings, 0.21 4125 0.96 0.04\n",
      "hat, 0.04 839 0.33 0.67\n",
      "necktie, 0.07 1399 0.01 0.99\n"
     ]
    }
   ],
   "source": [
    "for k, v in data['labels'].items():\n",
    "    gender = data['labels']['gender']\n",
    "    idx_female_l = np.where(np.logical_and(gender == 0, v == 1))[0]\n",
    "    idx_male_l = np.where(np.logical_and(gender == 1, v == 1))[0]\n",
    "    num = np.where(v)[0].shape[0]\n",
    "    print(f\"{k}, {np.mean(v):.2f} {num} {idx_female_l.shape[0]/sum(v) :.2f} {idx_male_l.shape[0]/sum(v) :.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb07394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_relevance_celeba(similarities, all_labels, desired_cat,  fname):\n",
    "\ttopks = [20, 50, 100]\n",
    "\tquery_dict = {}\n",
    "\tfor  idx, k in enumerate(queries):\n",
    "\n",
    "\t\tquery_dict[k] = similarities[idx]\n",
    "\t\t\n",
    "\tsorted_idx = {}\n",
    "\tfor k in query_dict.keys():\n",
    "\t\ts = np.asarray(query_dict[k])\n",
    "\t\tind_sorted = s.argsort()\n",
    "\t\tsorted_idx[k] = []\n",
    "\t\tfor topk in topks:\n",
    "\t\t\tind = ind_sorted[-topk:][::-1]\n",
    "\t\t\tsorted_idx[k].append(ind)\n",
    "\n",
    "\tcol_test = ['Query', 'precision_top_20', 'precision_top_50', 'precision_top_100']\n",
    "\tdf_skews = pd.DataFrame(columns = col_test)\n",
    "    \n",
    "        \n",
    "\tfor k_idx, (k, v) in enumerate(sorted_idx.items()):\n",
    "\t\t\n",
    "\t\trow = [k]\n",
    "\t\tprint_ = False\n",
    "\t\tif k_idx == 0:\n",
    "\t\t\tprint_ = False\n",
    "\t\tfor topi, v_ in enumerate(v):\n",
    "\t\t\t# print(k, len(v_))\n",
    "\t\t\t# check the categories of the retrieved\n",
    "\t\t\t\n",
    "\t\t\trow.append(np.sum(all_labels\n",
    "                              [desired_cat[k_idx]][v_])/ topks[topi]) #np.sum(all_labels[desired_cat[k_idx]])\n",
    "\t\tdf_row = pd.DataFrame([row], columns= col_test)\n",
    "\t\tdf_skews = df_skews.append(df_row, ignore_index=True)\n",
    "\n",
    "\tdf_skews = df_skews.round(2)\n",
    "\tdf_skews.to_csv(f\"../results_csv/{fname}_relevance.csv\")\n",
    "\tprint(df_skews)\n",
    "\treturn df_skews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5ba8cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['black_hair', 'blond_hair', 'brown_hair', 'glasses', 'smiling', 'wavy_hair', 'earrings', 'hat', 'necktie']\n"
     ]
    }
   ],
   "source": [
    "desired_labels = list(data['labels'].keys())\n",
    "desired_labels.remove('gender')\n",
    "\n",
    "print(desired_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bd43215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black_hair 5422\n"
     ]
    }
   ],
   "source": [
    "print(desired_labels[0], np.sum(data['labels'][desired_labels[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae9ed0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [  \"a person with black hair\", \"a person with blond hair\", \\\n",
    "           \"a person with brown hair\",\n",
    "            \"a person with glasses\", \\\n",
    "            \\\n",
    "             \"a person who is smiling\", \\\n",
    "#            \"a person with straight hair\",\\\n",
    "           \"a person with wavy hair\", \\\n",
    "           \"a person wearing earrings\", \"a person wearing a hat\", \\\n",
    "           \"a person wearing a tie\"   \n",
    "                         \n",
    "          ]\n",
    "#\"a person with straight hair\", \n",
    "#            \"a person with wavy hair\", \n",
    "#\"a person with a pointy nose\", \"a person with an oval face\",\"a person with high cheekbones\",, \n",
    "#             \"a person wearing a necklace\" , \"a person with a double chin\",\n",
    "#\"a person with arched eyebrows\", \\\n",
    "#            \"a person with bags under the eyes\", \"a person with big lips\",  \\\n",
    "#            \"a person with a big nose\",\n",
    "# \"a person with bushy eyebrows\", \"a chubby person\",\\\n",
    "\n",
    "text_tokens = clip.tokenize([\"a photo of \" + desc for desc in queries]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens)#.float()\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# all_features_val /= all_features_val.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * data['features'] @ text_features.T).cpu().numpy().astype(np.float64).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "486c2a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Query  precision_top_20  precision_top_50  \\\n",
      "0   a person with black hair              0.95              0.88   \n",
      "1   a person with blond hair              0.60              0.62   \n",
      "2   a person with brown hair              0.50              0.54   \n",
      "3      a person with glasses              1.00              1.00   \n",
      "4    a person who is smiling              1.00              1.00   \n",
      "5    a person with wavy hair              0.90              0.90   \n",
      "6  a person wearing earrings              0.65              0.70   \n",
      "7     a person wearing a hat              0.95              0.98   \n",
      "8     a person wearing a tie              0.60              0.66   \n",
      "\n",
      "   precision_top_100  \n",
      "0               0.84  \n",
      "1               0.70  \n",
      "2               0.57  \n",
      "3               0.98  \n",
      "4               1.00  \n",
      "5               0.90  \n",
      "6               0.72  \n",
      "7               0.97  \n",
      "8               0.56  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>precision_top_20</th>\n",
       "      <th>precision_top_50</th>\n",
       "      <th>precision_top_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a person with black hair</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a person with blond hair</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a person with brown hair</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a person with glasses</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a person who is smiling</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a person with wavy hair</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a person wearing earrings</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a person wearing a hat</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a person wearing a tie</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Query  precision_top_20  precision_top_50  \\\n",
       "0   a person with black hair              0.95              0.88   \n",
       "1   a person with blond hair              0.60              0.62   \n",
       "2   a person with brown hair              0.50              0.54   \n",
       "3      a person with glasses              1.00              1.00   \n",
       "4    a person who is smiling              1.00              1.00   \n",
       "5    a person with wavy hair              0.90              0.90   \n",
       "6  a person wearing earrings              0.65              0.70   \n",
       "7     a person wearing a hat              0.95              0.98   \n",
       "8     a person wearing a tie              0.60              0.66   \n",
       "\n",
       "   precision_top_100  \n",
       "0               0.84  \n",
       "1               0.70  \n",
       "2               0.57  \n",
       "3               0.98  \n",
       "4               1.00  \n",
       "5               0.90  \n",
       "6               0.72  \n",
       "7               0.97  \n",
       "8               0.56  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_relevance_celeba(similarity, data['labels'], desired_labels, \"orig_celeba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0e0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.91s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.67s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████████████████████████████████████▊                                                                                                                 | 283/828 [07:55<15:04,  1.66s/it]"
     ]
    }
   ],
   "source": [
    "projection_GT,projection_inferred, MI_GT, MI_inferred = ut.calculate_projections_coco(model, preprocess, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1767913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======== Running Fair pca G.T on the model ============== \")\n",
    "\n",
    "for attr in ['gender']:\n",
    "    \n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of {word}\") for word in queries]).to(device)\n",
    "#     text_inputs = clip.tokenize([\"a photo of \" + desc for desc in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    projection_train = projection_GT\n",
    "    all_features_val_transf = projection_train.just_transform(data['features'].cpu().numpy().astype(np.float64))\n",
    "    text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "    similarity = (100.0 * all_features_val_transf @ text_features_pca.T).T\n",
    "    retrieval_fpca_gt = run_relevance_celeba(similarity, data['labels'], desired_labels, \"fpca_gt_celeba\")\n",
    "#     print(retrieval_fpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300246b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======== Running Fair pca INF on the model ============== \")\n",
    "\n",
    "for attr in ['gender']:\n",
    "    \n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    projection_train = projection_inferred\n",
    "    all_features_val_transf = projection_train.just_transform(data['features'].cpu().numpy().astype(np.float64))\n",
    "    text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "    similarity = (100.0 * all_features_val_transf @ text_features_pca.T).T\n",
    "    retrieval_fpca_inf = run_relevance_celeba(similarity, data['labels'], desired_labels, \"fpca_inf_celeba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in ['gender']:\n",
    "    \n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features.cpu().numpy().astype(np.float64)\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_GT#[attr]\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"..... {num_clip}.........\")\n",
    "        \n",
    "        text_features_mi =text_features[:, mis[:num_clip]]\n",
    "        image_features_val = data['features'].cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "        similarity = (100.0 * image_features_val @ text_features_mi.T).T \n",
    "        run_relevance_celeba(similarity, data['labels'], desired_labels, f\"MI_gt{num_clip}_celeba\")        \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in ['gender']:\n",
    "    \n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features.cpu().numpy().astype(np.float64)\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_inferred#[attr]\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"..... {num_clip}.........\")\n",
    "        \n",
    "        text_features_mi =text_features[:, mis[:num_clip]]\n",
    "        image_features_val = data['features'].cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "        similarity = (100.0 * image_features_val @ text_features_mi.T).T \n",
    "        run_relevance_celeba(similarity, data['labels'], desired_labels, f\"MI_inf{num_clip}_celeba\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ce828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
