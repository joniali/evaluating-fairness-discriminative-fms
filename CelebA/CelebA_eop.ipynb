{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf02a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CelebA\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import copy\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import utils as ut\n",
    "import importlib\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a009cab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/mnt/efs/fairclip/FinalCode/CelebA/../utils.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "765a45c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:1\" #if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c340f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1628/1628 [15:10<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error of predicting gender train = 0.01\n"
     ]
    }
   ],
   "source": [
    "projection_GT,projection_inferred, MI_GT, MI_inferred = ut.calculate_projections_celeba(model, preprocess, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ca2ba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:48<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "data = ut.get_CelebA(\"test\", model, preprocess, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4edb1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_tasks = {}\n",
    "classification_tasks['gender'] = [\n",
    "    \n",
    "\n",
    "    ('not wearing glasses','wearing glasses'),\n",
    "    ('not wearing a necklace', 'wearing a necklace'),\n",
    "    ('with straight hair', 'with wavy hair'),\n",
    "\n",
    "]\n",
    "gt_label = [ 'glasses', 'necklace',  'wavy_hair']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1d2994c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "('not wearing glasses', 'wearing glasses') 0.98 0.06397154593728083\n",
      "('not wearing a necklace', 'wearing a necklace') 0.74 0.239655345155796\n",
      "('with straight hair', 'with wavy hair') 0.59 0.5026049494038674\n",
      "                                                Female      Male  Disparity  \\\n",
      "(not wearing glasses, wearing glasses)        0.946372  0.790123   0.156249   \n",
      "(not wearing a necklace, wearing a necklace)  0.447148  0.105691   0.341457   \n",
      "(with straight hair, with wavy hair)          0.580656  0.852254   0.271598   \n",
      "\n",
      "                                              Accuracy  \n",
      "(not wearing glasses, wearing glasses)            0.98  \n",
      "(not wearing a necklace, wearing a necklace)      0.74  \n",
      "(with straight hair, with wavy hair)              0.59  \n"
     ]
    }
   ],
   "source": [
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    accs = []\n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a person {word}.\") for word in task]).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * data['features'] @ text_features.T).softmax(dim=-1).cpu().numpy().astype(np.float64)\n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        accs.append(np.around(np.mean(predictions == data['labels'][gt_label[cc]]),2))\n",
    "        print(task,np.around(np.mean(predictions == data['labels'][gt_label[cc]]),2), np.mean(predictions) )\n",
    "        for ell in range(2):\n",
    "#             print(ell)\n",
    "            idx_ = np.where(data['labels']['gender'] == ell)\n",
    "            \n",
    "#             temp[cc, ell] = np.around(np.mean(predictions[idx_] == data['labels'][gt_label[cc]][idx_]),2)\n",
    "            \n",
    "            \n",
    "            idx_tp = np.where(np.logical_and(data['labels'][gt_label[cc]][idx_] == 1, predictions[idx_] == 1))[0]\n",
    "            \n",
    "            pos_ = np.where(data['labels'][gt_label[cc]][idx_] == 1)[0].shape[0]\n",
    "            temp[cc, ell] = idx_tp.shape[0]/ pos_\n",
    "            \n",
    "#             temp[cc, ell] = 1 - np.around(np.mean(predictions[data['labels']['gender']== ell] != data['labels'][]),2)\n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = abs(temp['Male'] - temp['Female'])\n",
    "        temp['Accuracy'] = accs\n",
    "    temp.to_csv(f\"../results_csv/{attr}_celeba_clf_orig.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e928f4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4139814366.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    Numbers are the mean prediction rate for the first word when classifying into the two words\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
    "# Numbers are the mean prediction rate for the first word when classifying into the two words\n",
    "# ('not wearing glasses', 'wearing glasses') 0.98 0.06397154593728083\n",
    "# ('not wearing a necklace', 'wearing a necklace') 0.74 0.239655345155796\n",
    "# ('with straight hair', 'with wavy hair') 0.59 0.5026049494038674\n",
    "#                                                 Female      Male  Disparity  \\\n",
    "# (not wearing glasses, wearing glasses)        0.946372  0.790123   0.156249   \n",
    "# (not wearing a necklace, wearing a necklace)  0.447148  0.105691   0.341457   \n",
    "# (with straight hair, with wavy hair)          0.580656  0.852254   0.271598   \n",
    "\n",
    "#                                               Accuracy  \n",
    "# (not wearing glasses, wearing glasses)            0.98  \n",
    "# (not wearing a necklace, wearing a necklace)      0.74  \n",
    "# (with straight hair, with wavy hair)              0.59 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72086c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "('not wearing glasses', 'wearing glasses') 0.98 0.07524296162709147\n",
    "('not wearing a necklace', 'wearing a necklace') 0.71 0.3014727983168019\n",
    "('with straight hair', 'with wavy hair') 0.59 0.5050596132652039\n",
    "                                                Female      Male  Disparity  \\\n",
    "(not wearing glasses, wearing glasses)        0.968454  0.881687   0.086767   \n",
    "(not wearing a necklace, wearing a necklace)  0.545247  0.186992   0.358255   \n",
    "(with straight hair, with wavy hair)          0.593014  0.858932   0.265918   \n",
    "\n",
    "                                              Accuracy  \n",
    "(not wearing glasses, wearing glasses)            0.98  \n",
    "(not wearing a necklace, wearing a necklace)      0.71  \n",
    "(with straight hair, with wavy hair)              0.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "341d63a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "('not wearing glasses', 'wearing glasses') 0.99 0.05901212303376415 0.985622683097886\n",
      "('not wearing a necklace', 'wearing a necklace') 0.81 0.08726580502955615 0.8128944995491434\n",
      "('with straight hair', 'with wavy hair') 0.62 0.5875162809337742 0.6198276725778981\n",
      "                                                Female      Male  Disparity  \\\n",
      "(not wearing glasses, wearing glasses)        0.927445  0.818930   0.108515   \n",
      "(not wearing a necklace, wearing a necklace)  0.136502  0.170732   0.034230   \n",
      "(with straight hair, with wavy hair)          0.782336  0.797162   0.014825   \n",
      "\n",
      "                                              Accuracy  \n",
      "(not wearing glasses, wearing glasses)            0.99  \n",
      "(not wearing a necklace, wearing a necklace)      0.81  \n",
      "(with straight hair, with wavy hair)              0.62  \n"
     ]
    }
   ],
   "source": [
    "# FPCA GT\n",
    "from scipy.special import softmax\n",
    "\n",
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    accs = []\n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a person {word}.\") for word in task]).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        projection_train = projection_GT[attr]\n",
    "        all_features_val_transf = projection_train.just_transform(data['features'].cpu().numpy().astype(np.float64))\n",
    "        text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "        similarity = softmax(100.0 * np.matmul(all_features_val_transf, np.transpose(text_features_pca)),axis=1)\n",
    "        \n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        print(task,np.around(np.mean(predictions == data['labels'][gt_label[cc]]),2), np.mean(predictions), accuracy_score(predictions,data['labels'][gt_label[cc]]))\n",
    "        accs.append(np.around(np.mean(predictions == data['labels'][gt_label[cc]]),2))\n",
    "        for ell in range(2):\n",
    "            idx_ = np.where(data['labels']['gender'] == ell)\n",
    "            \n",
    "#             temp[cc, ell] = np.around(np.mean(predictions[idx_] == data['labels'][gt_label[cc]][idx_]),2)\n",
    "            \n",
    "            \n",
    "            idx_tp = np.where(np.logical_and(data['labels'][gt_label[cc]][idx_] == 1, predictions[idx_] == 1))[0]\n",
    "            \n",
    "            pos_ = np.where(data['labels'][gt_label[cc]][idx_] == 1)[0].shape[0]\n",
    "            temp[cc, ell] = idx_tp.shape[0]/ pos_\n",
    "            \n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = abs(temp['Male'] - temp['Female'])\n",
    "        temp['Accuracy'] = accs\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_celeba_clf_fpca_gt.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(temp.Disparity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd78f019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "('not wearing glasses', 'wearing glasses') 0.99 0.05901212303376415\n",
      "('not wearing a necklace', 'wearing a necklace') 0.81 0.08726580502955615\n",
      "('with straight hair', 'with wavy hair') 0.62 0.5875663761146178\n",
      "                                                Female      Male  Disparity  \\\n",
      "(not wearing glasses, wearing glasses)        0.927445  0.818930   0.108515   \n",
      "(not wearing a necklace, wearing a necklace)  0.136502  0.170732   0.034230   \n",
      "(with straight hair, with wavy hair)          0.782831  0.795492   0.012662   \n",
      "\n",
      "                                              Accuracy  \n",
      "(not wearing glasses, wearing glasses)            0.99  \n",
      "(not wearing a necklace, wearing a necklace)      0.81  \n",
      "(with straight hair, with wavy hair)              0.62  \n"
     ]
    }
   ],
   "source": [
    "# FPCA INF\n",
    "from scipy.special import softmax\n",
    "\n",
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    accs =[]\n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a person {word}.\") for word in task]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        projection_train = projection_inferred[attr]\n",
    "        all_features_val_transf = projection_train.just_transform(data['features'].cpu().numpy().astype(np.float64))\n",
    "        text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "        similarity = softmax(100.0 * np.matmul(all_features_val_transf, np.transpose(text_features_pca)),axis=1)\n",
    "        \n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        accs.append(np.around(np.mean(predictions == data['labels'][gt_label[cc]]),2))\n",
    "        print(task,np.around(np.mean(predictions == data['labels'][gt_label[cc]]),2), np.mean(predictions) )\n",
    "        \n",
    "        for ell in range(2):\n",
    "            idx_ = np.where(data['labels']['gender'] == ell)\n",
    "            \n",
    "#             temp[cc, ell] = np.around(np.mean(predictions[idx_] == data['labels'][gt_label[cc]][idx_]),2)\n",
    "            \n",
    "            \n",
    "            idx_tp = np.where(np.logical_and(data['labels'][gt_label[cc]][idx_] == 1, predictions[idx_] == 1))[0]\n",
    "            \n",
    "            pos_ = np.where(data['labels'][gt_label[cc]][idx_] == 1)[0].shape[0]\n",
    "            temp[cc, ell] = idx_tp.shape[0]/ pos_\n",
    "            \n",
    "            \n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = abs(temp['Male'] - temp['Female'])\n",
    "        temp['Accuracy'] = accs\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_celeba_clf_fpca_inf.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e12ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(temp.Disparity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f2c34ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running CLF MI G.T on the model ============== \n",
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "----------- 400--------------\n",
      "('not wearing glasses', 'wearing glasses') 0.95 0.013926460274521591\n",
      "('not wearing a necklace', 'wearing a necklace') 0.86 5.009518084360284e-05\n",
      "('with straight hair', 'with wavy hair') 0.6 0.6149183448552249\n",
      "                                                Female      Male  Disparity  \\\n",
      "(not wearing glasses, wearing glasses)        0.186120  0.219136   0.033016   \n",
      "(not wearing a necklace, wearing a necklace)  0.000000  0.000000   0.000000   \n",
      "(with straight hair, with wavy hair)          0.795848  0.820534   0.024686   \n",
      "\n",
      "                                              Accuracy  \n",
      "(not wearing glasses, wearing glasses)            0.95  \n",
      "(not wearing a necklace, wearing a necklace)      0.86  \n",
      "(with straight hair, with wavy hair)              0.60  \n",
      "0.019234133831075407\n",
      "----------- 256--------------\n",
      "('not wearing glasses', 'wearing glasses') 0.64 0.4115319106301974\n",
      "('not wearing a necklace', 'wearing a necklace') 0.27 0.8257188658451057\n",
      "('with straight hair', 'with wavy hair') 0.67 0.38949003105901214\n",
      "                                                Female      Male  Disparity  \\\n",
      "(not wearing glasses, wearing glasses)        0.946372  0.913580   0.032792   \n",
      "(not wearing a necklace, wearing a necklace)  0.869962  0.747967   0.121994   \n",
      "(with straight hair, with wavy hair)          0.569946  0.614357   0.044412   \n",
      "\n",
      "                                              Accuracy  \n",
      "(not wearing glasses, wearing glasses)            0.64  \n",
      "(not wearing a necklace, wearing a necklace)      0.27  \n",
      "(with straight hair, with wavy hair)              0.67  \n",
      "0.06639937571337588\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running CLF MI G.T on the model ============== \")\n",
    "\n",
    "for attr in ['gender']:\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_GT[attr]\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"----------- {num_clip}--------------\")\n",
    "        temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "        accs = []\n",
    "        for cc, task in enumerate(classification_tasks[attr]):\n",
    "            text_inputs = torch.cat([clip.tokenize(f\"a photo of a person {word}.\") for word in task]).to(device)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_inputs)\n",
    "            text_features_mi =text_features.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            image_features_val = data['features'].cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            similarity = softmax(100.0 * np.matmul(image_features_val, np.transpose(text_features_mi)),axis=1)\n",
    "\n",
    "            predictions = np.argmax(similarity,axis=1)\n",
    "            print(task,np.around(np.mean(predictions == data['labels'][gt_label[cc]]),2), np.mean(predictions) )\n",
    "            accs.append(np.around(np.mean(predictions == data['labels'][gt_label[cc]]),2))\n",
    "            \n",
    "            for ell in range(2):\n",
    "                idx_ = np.where(data['labels']['gender'] == ell)\n",
    "            \n",
    "#             temp[cc, ell] = np.around(np.mean(predictions[idx_] == data['labels'][gt_label[cc]][idx_]),2)\n",
    "            \n",
    "            \n",
    "                idx_tp = np.where(np.logical_and(data['labels'][gt_label[cc]][idx_] == 1, predictions[idx_] == 1))[0]\n",
    "\n",
    "                pos_ = np.where(data['labels'][gt_label[cc]][idx_] == 1)[0].shape[0]\n",
    "                temp[cc, ell] = idx_tp.shape[0]/ pos_\n",
    "                \n",
    "        columns= ['Female', 'Male']\n",
    "        temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "        if attr == 'gender':\t  \n",
    "            temp['Disparity'] = abs(temp['Male'] - temp['Female'])\n",
    "            temp['Accuracy'] = accs\n",
    "        elif attr == 'race':\n",
    "            temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "        temp.to_csv(f\"../results_csv/{attr}_celeba_clf_MI_gt{num_clip}.csv\")\n",
    "        print(temp)\n",
    "        print(np.mean(temp.Disparity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2fd0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(temp.Disparity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e066d4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running CLF MI INF on the model ============== \n",
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "----------- 400--------------\n",
      "                                                Female      Male  Disparity  \\\n",
      "(not wearing glasses, wearing glasses)        0.186120  0.204733   0.018613   \n",
      "(not wearing a necklace, wearing a necklace)  0.000000  0.000000   0.000000   \n",
      "(with straight hair, with wavy hair)          0.790575  0.824708   0.034133   \n",
      "\n",
      "                                              Accuracy  \n",
      "(not wearing glasses, wearing glasses)            0.95  \n",
      "(not wearing a necklace, wearing a necklace)      0.86  \n",
      "(with straight hair, with wavy hair)              0.60  \n",
      "----------- 256--------------\n",
      "                                                Female      Male  Disparity  \\\n",
      "(not wearing glasses, wearing glasses)        0.949527  0.910494   0.039033   \n",
      "(not wearing a necklace, wearing a necklace)  0.867681  0.739837   0.127843   \n",
      "(with straight hair, with wavy hair)          0.572417  0.617696   0.045279   \n",
      "\n",
      "                                              Accuracy  \n",
      "(not wearing glasses, wearing glasses)            0.65  \n",
      "(not wearing a necklace, wearing a necklace)      0.27  \n",
      "(with straight hair, with wavy hair)              0.66  \n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running CLF MI INF on the model ============== \")\n",
    "\n",
    "for attr in ['gender']:\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_inferred[attr]\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"----------- {num_clip}--------------\")\n",
    "        temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "        accs = []\n",
    "        \n",
    "        for cc, task in enumerate(classification_tasks[attr]):\n",
    "            text_inputs = torch.cat([clip.tokenize(f\"a photo of a person {word}.\") for word in task]).to(device)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_inputs)\n",
    "            text_features_mi =text_features.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            image_features_val = data['features'].cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            similarity = softmax(100.0 * np.matmul(image_features_val, np.transpose(text_features_mi)),axis=1)\n",
    "            \n",
    "            \n",
    "            predictions = np.argmax(similarity,axis=1)\n",
    "            accs.append(np.around(np.mean(predictions == data['labels'][gt_label[cc]]),2))\n",
    "            for ell in range(2):\n",
    "                idx_ = np.where(data['labels']['gender'] == ell)\n",
    "            \n",
    "#             temp[cc, ell] = np.around(np.mean(predictions[idx_] == data['labels'][gt_label[cc]][idx_]),2)\n",
    "            \n",
    "            \n",
    "                idx_tp = np.where(np.logical_and(data['labels'][gt_label[cc]][idx_] == 1, predictions[idx_] == 1))[0]\n",
    "\n",
    "                pos_ = np.where(data['labels'][gt_label[cc]][idx_] == 1)[0].shape[0]\n",
    "                temp[cc, ell] = idx_tp.shape[0]/ pos_\n",
    "                \n",
    "        columns= ['Female', 'Male']\n",
    "        temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "        if attr == 'gender':\t  \n",
    "            temp['Disparity'] = abs(temp['Male'] - temp['Female'])\n",
    "            temp['Accuracy'] = accs\n",
    "            \n",
    "        elif attr == 'race':\n",
    "            temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "        temp.to_csv(f\"../results_csv/{attr}_celeba_clf_MI_inf{num_clip}.csv\")\n",
    "        print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95778f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(temp.Disparity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef907808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
