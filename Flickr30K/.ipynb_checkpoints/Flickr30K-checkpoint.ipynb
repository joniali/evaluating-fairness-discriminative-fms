{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2c5c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "from PIL import Image\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "# from IPython.display import Image, display\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "clip.available_models()\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import flickr_dataset as fl \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "from scipy import stats\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import utils as ut\n",
    "import importlib\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ac546b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/mnt/efs/fairclip/FinalCode/Flickr30K/../utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8a32a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afbda386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" #if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac67fa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 0\n",
      "15507 15507\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(fl)\n",
    "flickrData = fl.MyFlickr30k('../../flicker30k-images/flickr30k-images', '../../flicker30k-captions/results_20130124.token',None, transform=preprocess)\n",
    "train_size = int(0.5 * len(flickrData))\n",
    "test_size = len(flickrData) - train_size\n",
    "torch.manual_seed(0)\n",
    "set_seed(0)\n",
    "flickrData_train, flickrData_test = torch.utils.data.random_split(flickrData, [train_size, test_size])\n",
    "print(len(flickrData_test), len(flickrData_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419f00f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15507 15507\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(fl)\n",
    "flickrData_orig = fl.MyFlickr30k('../../flicker30k-images/flickr30k-images', '../../flicker30k-captions/results_20130124.token',None)\n",
    "train_size = int(0.5 * len(flickrData))\n",
    "test_size = len(flickrData_orig) - train_size\n",
    "torch.manual_seed(0)\n",
    "flickrData_train_orig, flickrData_test_orig = torch.utils.data.random_split(flickrData_orig, [train_size, test_size])\n",
    "print(len(flickrData_test_orig), len(flickrData_train_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21dc58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077858f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|█████████████████████████████████████████████████████████████▌                  | 120/156 [01:28<00:25,  1.39it/s]"
     ]
    }
   ],
   "source": [
    "all_features_train, all_labels_captions_train, all_labels_gender_train = ut.get_features_flickr(flickrData_train, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02902e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_test, all_labels_captions_test, all_labels_gender_test = ut.get_features_flickr(flickrData_test, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ab555785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Karpathy We do not use the Karpathy split but we provide the code. Just uncomment the following lines\n",
    "# and change the variable all_features_test to all_features_test_k \n",
    "# importlib.reload(fl)\n",
    "# flickrData = fl.MyFlickr30k('flicker30k-images/flickr30k-images', 'flicker30k-captions/results_20130124.token','test', transform=preprocess)\n",
    "# all_features_test_k, all_labels_captions_test_k, all_labels_gender_test_k = ut.get_features_flickr(flickrData, model, device)\n",
    "# all_features_test_k /= all_features_test_k.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be84773",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_test /= all_features_test.norm(dim=-1, keepdim=True)\n",
    "all_features_train /= all_features_train.norm(dim=-1, keepdim=True)\n",
    "protected_attribute = {'gender':all_labels_gender_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db79041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_gender_queries = 5\n",
    "\n",
    "queries = [ \"doctor\", \"nurse\", \"secretary\", 'boss', 'lawyer', 'paralegal']\n",
    "\n",
    "text_inputs = torch.cat([clip.tokenize(f\"This is a photo of a {word}\") for word in queries]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)#.float()\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "similarity = (100.0 * all_features_test @ text_features.T).cpu().numpy().astype(np.float64).T\n",
    "ut.calc_similarity_diff('orig_flickr', 'gender', queries, protected_attribute , {0: 'Female', 1:'Male'},similarity)\n",
    "ut.run_anova(queries, all_labels_gender_test, similarity, 'orig_flickr', skip_att = 2)\n",
    "ut.run_skew(queries, all_labels_gender_test, similarity, 'orig_flickr', [10, 20, 30], skip_attr = 2)\n",
    "ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, 'orig_flickr', [10, 20, 30], skip_attr = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1170c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b81985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall test Karpathy\n",
    "# import time\n",
    "# start = time.time()\n",
    "# flat_captions = all_labels_captions_test_k.flatten()\n",
    "# similarity = []\n",
    "# for cap in flat_captions:\n",
    "#     #cap[:300]\n",
    "#     text_inputs = clip.tokenize(f\"a photo of {cap}\", truncate = True).to(device)\n",
    "# # for cap in all_labels_captions_test_k: #flat_captions:\n",
    "    \n",
    "# #     text_inputs = clip.tokenize(f\"a photo of {''.join(cap)}\", truncate = True).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         text_features = model.encode_text(text_inputs)#.float()\n",
    "#     text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#     similarity.append((100.0 * all_features_test_k @ text_features.T).cpu().numpy().astype(np.float64))\n",
    "# similarity = np.asarray(similarity).squeeze()\n",
    "# print(similarity.shape)\n",
    "# # similarity = np.asarray(similarity).reshape((77535,15507))\n",
    "# ut.calculate_recall(similarity, \"orig_flickr_karp\")\n",
    "# end = time.time()\n",
    "# print(end - start)\n",
    "# using all the descriptions joined \n",
    "# mean_top_1  mean_top_5  mean_top_10\n",
    "# 0        0.79        0.97         0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3de8fdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77535, 15507)\n",
      "   mean_top_1  mean_top_5  mean_top_10\n",
      "0       0.315       0.548        0.647\n",
      "667.9809448719025\n"
     ]
    }
   ],
   "source": [
    "#recall \n",
    "importlib.reload(ut)\n",
    "import time\n",
    "start = time.time()\n",
    "flat_captions = all_labels_captions_test.flatten()\n",
    "similarity = []\n",
    "for cap in flat_captions:\n",
    "    #cap[:300]\n",
    "    text_inputs = clip.tokenize(f\"a photo of {cap}\", truncate = True).to(device)\n",
    "# for cap in all_labels_captions_test: #flat_captions:\n",
    "    \n",
    "#     text_inputs = clip.tokenize(f\"a photo of {''.join(cap)}\", truncate = True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)#.float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity.append((100.0 * all_features_test @ text_features.T).cpu().numpy().astype(np.float64))\n",
    "similarity = np.asarray(similarity).squeeze()\n",
    "print(similarity.shape)\n",
    "# similarity = np.asarray(similarity).reshape((77535,15507 ))\n",
    "ut.calculate_recall(similarity, \"orig_flickr\")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5a865e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77535,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.315       0.548        0.647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_tasks = {}\n",
    "classification_tasks['gender'] = [\n",
    "    ('criminal','innocent person'),\n",
    "    ('nurse','doctor'),\n",
    "    ('prostitute','night owl'),\n",
    "    ('CEO','secretary'),\n",
    "    ('leader','follower'),\n",
    "    ('lawyer','paralegal'),\n",
    "    ('secretary','board member'),\n",
    "    ('convict',\t'exoneree'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed3bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * all_features_test @ text_features.T).softmax(dim=-1).cpu().numpy().astype(np.float64)\n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        for ell in range(2):\n",
    "#             print(ell)\n",
    "            temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_flickr_clf_orig.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_GT,projection_inferred, MI_GT, MI_inferred = ut.calculate_projections_flickr(all_features_train, all_labels_gender_train, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10468a",
   "metadata": {},
   "source": [
    "# Fair PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d9287",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======== Running Fair pca G.T on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    projection_train = projection_GT[attr]\n",
    "    all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "    text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "    similarity = (100.0 * all_features_val_transf @ text_features_pca.T).T\n",
    "    ut.calc_similarity_diff('fpca_gt_flickr', 'gender',queries, protected_attribute , {0: 'Female', 1:'Male'},similarity)\n",
    "    ut.run_anova(queries, all_labels_gender_test, similarity, 'fpca_gt_flickr', skip_att = 2)\n",
    "    ut.run_skew(queries, all_labels_gender_test, similarity, 'fpca_gt_flickr', [10, 20, 30],skip_attr = 2)\n",
    "    ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, 'fpca_gt_flickr',[10, 20, 30], skip_attr = 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a72a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======== Running CLF Fair pca G.T on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        projection_train = projection_GT[attr]\n",
    "        all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "        text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "        similarity = softmax(100.0 * np.matmul(all_features_val_transf, np.transpose(text_features_pca)),axis=1)\n",
    "        \n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        for ell in range(2):\n",
    "#             print(ell)\n",
    "            temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_flickr_clf_fpca_gt.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======== Running Recall Fair pca with G.T attribute ============== \")\n",
    "import time\n",
    "start = time.time()\n",
    "flat_captions = all_labels_captions_test.flatten()\n",
    "similarity = []\n",
    "projection_train = projection_GT['gender']\n",
    "all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "\n",
    "for cap in flat_captions:\n",
    "    text_inputs = clip.tokenize(f\"a photo of {cap}\", truncate = True).to(device)\n",
    "# for cap in all_labels_captions_test: #flat_captions:\n",
    "    \n",
    "#     text_inputs = clip.tokenize(f\"a photo of {''.join(cap)}\", truncate = True).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)#.float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "    similarity.append(100.0 * all_features_val_transf @ text_features_pca.T)\n",
    "similarity = np.asarray(similarity).squeeze()#reshape((77535,15507 ))\n",
    "ut.calculate_recall(similarity, \"fpca_gt_flickr\")\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebec9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======== Running Fair pca INF on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    \n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    projection_train = projection_inferred[attr]\n",
    "    all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "    text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "    similarity = (100.0 * all_features_val_transf @ text_features_pca.T).T\n",
    "    ut.calc_similarity_diff('fpca_inf_flickr', 'gender', queries, protected_attribute , {0: 'Female', 1:'Male'},similarity)\n",
    "    ut.run_anova(queries, all_labels_gender_test, similarity, 'fpca_inf_flickr', skip_att = 2)\n",
    "    ut.run_skew(queries, all_labels_gender_test, similarity, 'fpca_inf_flickr',[10, 20, 30], skip_attr = 2)\n",
    "    ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, 'fpca_inf_flickr',[10, 20, 30], skip_attr = 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======== Running CLF Fair pca inf on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        projection_train = projection_inferred[attr]\n",
    "        all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "        text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "        similarity = softmax(100.0 * np.matmul(all_features_val_transf, np.transpose(text_features_pca)),axis=1)\n",
    "        \n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        for ell in range(2):\n",
    "#             print(ell)\n",
    "            temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_flickr_clf_fpca_inf.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======== Running Recall pca Inferred on the model ============== \")\n",
    "import time\n",
    "start = time.time()\n",
    "flat_captions = all_labels_captions_test.flatten()\n",
    "similarity = []\n",
    "projection_train = projection_inferred['gender']\n",
    "all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "\n",
    "for cap in flat_captions:\n",
    "    text_inputs = clip.tokenize(f\"a photo of {cap}\", truncate = True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)#.float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "    similarity.append(100.0 * all_features_val_transf @ text_features_pca.T)\n",
    "similarity = np.asarray(similarity).squeeze()#reshape((77535,15507 ))\n",
    "ut.calculate_recall(similarity, \"fpca_inf_flickr\")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c09b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "0       0.289       0.513        0.612"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4e239",
   "metadata": {},
   "source": [
    "# Clip-clip https://arxiv.org/abs/2109.05433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======== Running MI G.T on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    \n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features.cpu().numpy().astype(np.float64)\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_GT[attr]\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"..... {num_clip}.........\")\n",
    "        \n",
    "        text_features_mi =text_features[:, mis[:num_clip]]\n",
    "        image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "        similarity = (100.0 * image_features_val @ text_features_mi.T).T \n",
    "        ut.calc_similarity_diff(f'MI_gt{num_clip}_flickr','gender', queries, protected_attribute , {0: 'Female', 1:'Male'}, similarity)\n",
    "        ut.run_anova(queries, all_labels_gender_test, similarity, f'MI_gt{num_clip}_flickr', skip_att = 2)\n",
    "        ut.run_skew(queries, all_labels_gender_test, similarity, f'MI_gt{num_clip}_flickr', [10, 20, 30],skip_attr = 2)\n",
    "        ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, f'MI_gt{num_clip}_flickr', [10, 20, 30],skip_attr = 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b1a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======== Running MI G.T recall on the model ============== \")\n",
    "\n",
    "flat_captions = all_labels_captions_test.flatten()\n",
    "for attr in ['gender']:\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_GT[attr]\n",
    "    for num_clip in num_clip_s:\n",
    "#         import time\n",
    "        start = time.time()\n",
    "        \n",
    "        similarity = []\n",
    "        \n",
    "        \n",
    "        image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "        \n",
    "        for cap in flat_captions:\n",
    "            text_inputs = clip.tokenize(f\"a photo of {cap}\", truncate = True).to(device)\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_inputs)#.float()\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features.cpu().numpy().astype(np.float64)\n",
    "            text_features_mi =text_features[:, mis[:num_clip]]\n",
    "            similarity.append((100.0 * image_features_val @ text_features_mi.T).T)\n",
    "        similarity = np.asarray(similarity).squeeze()#.reshape((77535,15507 ))\n",
    "        ut.calculate_recall(similarity, f\"MI_gt{num_clip}_flickr\")\n",
    "        end = time.time()\n",
    "        print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb92199",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======== Running retrieval MI INF on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    \n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features.cpu().numpy().astype(np.float64)\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_inferred[attr]\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"..... {num_clip}.........\")\n",
    "        text_features_mi =text_features[:, mis[:num_clip]]\n",
    "        image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "        similarity = (100.0 * image_features_val @ text_features_mi.T).T \n",
    "        ut.calc_similarity_diff(f'MI_inf{num_clip}_flickr', 'gender',queries, protected_attribute , {0: 'Female', 1:'Male'}, similarity)\n",
    "        ut.run_anova(queries, all_labels_gender_test, similarity, f'MI_inf{num_clip}_flickr', skip_att = 2)\n",
    "        ut.run_skew(queries, all_labels_gender_test, similarity, f'MI_inf{num_clip}_flickr',[10, 20, 30],skip_attr = 2)\n",
    "        ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, f'MI_inf{num_clip}_flickr',[10, 20, 30],skip_attr = 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87fec137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running MI INF recall on the model ============== \n",
      "   mean_top_1  mean_top_5  mean_top_10\n",
      "0       0.229       0.422        0.514\n",
      "1767.8580169677734\n",
      "   mean_top_1  mean_top_5  mean_top_10\n",
      "0       0.149       0.304        0.384\n",
      "1111.21799826622\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running MI INF recall on the model ============== \")\n",
    "\n",
    "flat_captions = all_labels_captions_test.flatten()\n",
    "for attr in ['gender']:\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_inferred[attr]\n",
    "    for num_clip in num_clip_s:\n",
    "#         import time\n",
    "        start = time.time()\n",
    "        similarity = []\n",
    "        image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "        for cap in flat_captions:\n",
    "            text_inputs = clip.tokenize(f\"a photo of {cap}\", truncate = True).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_inputs)#.float()\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features.cpu().numpy().astype(np.float64)\n",
    "            text_features_mi =text_features[:, mis[:num_clip]]\n",
    "            similarity.append((100.0 * image_features_val @ text_features_mi.T).T)\n",
    "        similarity = np.asarray(similarity).squeeze()#.reshape((77535,15507 ))\n",
    "        ut.calculate_recall(similarity, f\"MI_inf{num_clip}_flickr\")\n",
    "        end = time.time()\n",
    "        print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a88633",
   "metadata": {},
   "outputs": [],
   "source": [
    "======== Running MI INF recall on the model ============== \n",
    "   mean_top_1  mean_top_5  mean_top_10\n",
    "0       0.229       0.421        0.514\n",
    "2881.425798892975\n",
    "   mean_top_1  mean_top_5  mean_top_10\n",
    "0       0.149       0.304        0.384\n",
    "2773.1723685264587"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======== Running CLF MI G.T on the model ============== \")\n",
    "\n",
    "for attr in ['gender']:\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_GT[attr]\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"----------- {num_clip}--------------\")\n",
    "        temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "        for cc, task in enumerate(classification_tasks[attr]):\n",
    "            text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_inputs)\n",
    "            text_features_mi =text_features.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            similarity = softmax(100.0 * np.matmul(image_features_val, np.transpose(text_features_mi)),axis=1)\n",
    "\n",
    "            predictions = np.argmax(similarity,axis=1)\n",
    "            for ell in range(2):\n",
    "    #             print(ell)\n",
    "                temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "        columns= ['Female', 'Male']\n",
    "        temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "        if attr == 'gender':\t  \n",
    "            temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "        elif attr == 'race':\n",
    "            temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "        temp.to_csv(f\"../results_csv/{attr}_flickr_clf_MI_gt{num_clip}.csv\")\n",
    "        print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767b3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe8dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======== Running CLF MI inf on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_inferred[attr]\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"----------- {num_clip}--------------\")\n",
    "        temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "        for cc, task in enumerate(classification_tasks[attr]):\n",
    "            text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_inputs)\n",
    "            text_features_mi =text_features.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            similarity = softmax(100.0 * np.matmul(image_features_val, np.transpose(text_features_mi)),axis=1)\n",
    "\n",
    "            predictions = np.argmax(similarity,axis=1)\n",
    "            for ell in range(2):\n",
    "    #             print(ell)\n",
    "                temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "        columns= ['Female', 'Male']\n",
    "        temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "        if attr == 'gender': \n",
    "            temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "        elif attr == 'race':\n",
    "            temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "        temp.to_csv(f\"../results_csv/{attr}_flickr_clf_MI_inf{num_clip}.csv\")\n",
    "        print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56b151",
   "metadata": {},
   "source": [
    "# Prompt method https://arxiv.org/abs/2203.11933"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049aae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../debias-vision-lang')\n",
    "import debias_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfa4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "deb_clip_model, deb_preprocess = debias_clip.load(\"ViT-B/16-gender\", device=device)\n",
    "deb_clip_model.eval()\n",
    "flickrData = fl.MyFlickr30k('../../flicker30k-images/flickr30k-images', '../../flicker30k-captions/results_20130124.token',None, transform=deb_preprocess)\n",
    "train_size = int(0.5 * len(flickrData))\n",
    "test_size = len(flickrData) - train_size\n",
    "torch.manual_seed(0)\n",
    "set_seed(0)\n",
    "flickrData_train, flickrData_test = torch.utils.data.random_split(flickrData, [train_size, test_size])\n",
    "all_features_test_deb, all_labels_captions_test_deb, all_labels_gender_test_deb = ut.get_features_flickr(flickrData_test, deb_clip_model, device)\n",
    "all_features_test_deb /= all_features_test_deb.norm(dim=-1, keepdim=True)\n",
    "flat_captions = all_labels_captions_test_deb.flatten()\n",
    "protected_attribute = {'gender':all_labels_gender_test_deb}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing bias in debias model\")\n",
    "\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in queries]).to(\"cpu\")\n",
    "deb_clip_model_cpu, deb_preprocess = debias_clip.load(\"ViT-B/16-gender\", device='cpu')\n",
    "deb_clip_model_cpu.eval()\n",
    "with torch.no_grad():\n",
    "#     deb_clip_model = deb_clip_model.to(\"cpu\") # didn't work! \n",
    "    text_features_deb = deb_clip_model_cpu.encode_text(text_inputs).to(torch.float16)\n",
    "    text_features_deb = text_features_deb.to(device)\n",
    "print(text_inputs.shape, text_features_deb.shape)\n",
    "text_features_deb /= text_features_deb.norm(dim=-1, keepdim=True)\n",
    "similarity_deb = (100.0 * all_features_test_deb @ text_features_deb.T).cpu().numpy().astype(np.float64).T\n",
    "ut.calc_similarity_diff(f'prompt_flickr','gender', queries,  protected_attribute, {0: 'Female', 1:'Male'}, similarity_deb)\n",
    "ut.run_anova(queries, all_labels_gender_test_deb, similarity_deb, f'prompt_flickr', skip_att = 2)\n",
    "ut.run_skew(queries, all_labels_gender_test_deb, similarity_deb, f'prompt_flickr',[10, 20, 30],skip_attr = 2)\n",
    "ut.run_retrieval_metric(queries, all_labels_gender_test_deb, similarity_deb, f'prompt_flickr',[10, 20, 30],skip_attr = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f15c7307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running Prompt recall on the model ============== \n",
      "Installing pretrained embedings\n",
      " best_ndkl_oai-clip-vit-b-16_neptune_run_OXVLB-317_model_e4_step_5334_embeddings.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4.73k/4.73k [00:00<00:00, 10.2MiB/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 77535/77535 [20:47<00:00, 62.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_top_1  mean_top_5  mean_top_10\n",
      "0       0.353       0.593         0.69\n",
      "1327.7051665782928\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running Prompt recall on the model ============== \")\n",
    "import time\n",
    "for attr in ['gender']:\n",
    "    start = time.time()\n",
    "\n",
    "    similarity = []\n",
    "\n",
    "    deb_clip_model_cpu, deb_preprocess = debias_clip.load(\"ViT-B/16-gender\", device='cpu')\n",
    "    deb_clip_model_cpu.eval()\n",
    "    for cap in tqdm(flat_captions):\n",
    "        text_inputs = clip.tokenize(f\"a photo of {cap}\", truncate = True).to(\"cpu\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_features = deb_clip_model_cpu.encode_text(text_inputs).to(torch.float16)\n",
    "            text_features = text_features.to(device)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "      \n",
    "      \n",
    "        similarity.append((100.0 * all_features_test_deb @ text_features.T).cpu().numpy().astype(np.float64).T)\n",
    "    similarity = np.asarray(similarity).squeeze()\n",
    "    ut.calculate_recall(similarity, f\"prompt_flickr\")\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "754e92a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77535, 15507)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_top_1  mean_top_5  mean_top_10\n",
    "0       0.352       0.594         0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeafd4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.29        0.52         0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b78890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.353       0.594         0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    deb_clip_model_cpu, deb_preprocess = debias_clip.load(\"ViT-B/16-gender\", device='cpu')\n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in task])#.to(device)\n",
    "        with torch.no_grad():\n",
    "#     deb_clip_model = deb_clip_model.to(\"cpu\") # didn't work! \n",
    "            text_features_deb = deb_clip_model_cpu.encode_text(text_inputs).to(torch.float16)\n",
    "            text_features_deb = text_features_deb.to(device)\n",
    "        text_features_deb /= text_features_deb.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * all_features_test_deb @ text_features_deb.T).softmax(dim=-1).cpu().numpy().astype(np.float64)\n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        for ell in range(2):\n",
    "#             print(ell)\n",
    "            temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test_deb==ell]),2)\n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_flickr_clf_prompt.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf0397d",
   "metadata": {},
   "source": [
    "# Explicit gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278bcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------- Run gendered ------------------\")\n",
    "word_list_gendered = []\n",
    "for word in queries:\n",
    "    word_list_gendered.append(f'male {word}')\n",
    "    word_list_gendered.append(f'female {word}')\n",
    "      \n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in word_list_gendered]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity_gendered = (100.0 * all_features_test @ text_features.T).cpu().numpy().astype(np.float64).T\n",
    "ut.run_skew_mixed(queries, similarity_gendered, all_labels_gender_test, 'gen_bln_flickr', [10,20,30], skip_attr = 2)\n",
    "ut.run_retrieval_metric_mixed(queries, similarity_gendered, all_labels_gender_test, 'gen_bln_flickr', [10,20,30], skip_attr = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "004dda2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(all_labels_gender_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "659d53c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "len(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "992a246a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ...,  0, -1, -2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels_gender_test - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa25d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
