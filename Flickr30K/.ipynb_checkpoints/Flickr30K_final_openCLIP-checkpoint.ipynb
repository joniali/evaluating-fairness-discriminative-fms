{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2c5c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "from PIL import Image\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "# from IPython.display import Image, display\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import flickr_dataset as fl  \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "from scipy import stats\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import utils as ut\n",
    "import importlib\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afbda386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model and reading datasets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (patchnorm_pre_ln): Identity()\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"loading model and reading datasets\")\n",
    "device = torch.device('cuda:1')\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16',device =device, pretrained='laion400m_e32')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac67fa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15507 15507\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(fl)\n",
    "flickrData = fl.MyFlickr30k('../../flicker30k-images/flickr30k-images', '../../flicker30k-captions/results_20130124.token',None, transform=preprocess)\n",
    "train_size = int(0.5 * len(flickrData))\n",
    "test_size = len(flickrData) - train_size\n",
    "torch.manual_seed(0)\n",
    "flickrData_train, flickrData_test = torch.utils.data.random_split(flickrData, [train_size, test_size])\n",
    "print(len(flickrData_test), len(flickrData_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "419f00f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15507 15507\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(fl)\n",
    "flickrData_orig = fl.MyFlickr30k('../../flicker30k-images/flickr30k-images', '../../flicker30k-captions/results_20130124.token',None)\n",
    "train_size = int(0.5 * len(flickrData))\n",
    "test_size = len(flickrData_orig) - train_size\n",
    "torch.manual_seed(0)\n",
    "flickrData_train_orig, flickrData_test_orig = torch.utils.data.random_split(flickrData_orig, [train_size, test_size])\n",
    "print(len(flickrData_test_orig), len(flickrData_train_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53dbc212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flickr_splits = pd.read_json(\"caption_datasets/dataset_flickr30k.json\")\n",
    "# flickr_splits = flickr_splits['images'].apply(pd.Series)\n",
    "# print(len(flickr_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f21dc58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/mnt/efs/fairclip/FinalCode/Flickr30K/../utils.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "077858f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [02:45<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "all_features_train, all_labels_captions_train, all_labels_gender_train = ut.get_features_flickr(flickrData_train, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c02902e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [02:36<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "all_features_test, all_labels_captions_test, all_labels_gender_test = ut.get_features_flickr(flickrData_test, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be84773",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_test /= all_features_test.norm(dim=-1, keepdim=True)\n",
    "all_features_train /= all_features_train.norm(dim=-1, keepdim=True)\n",
    "protected_attribute = {'gender':all_labels_gender_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8db79041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female   Male  Disparity\n",
      "doctor      10.51  10.92       0.41\n",
      "nurse       12.94   9.88      -3.06\n",
      "secretary   12.32   8.87      -3.45\n",
      "boss        15.51  17.13       1.62\n",
      "lawyer      10.15  10.48       0.33\n",
      "paralegal   11.74   8.97      -2.77\n",
      "-------------------------------------------------------------------\n",
      "       Query         stat           pval\n",
      "0     doctor    27.419193   1.637975e-07\n",
      "1      nurse  1396.748582  1.068980e-305\n",
      "2  secretary  1729.302014   0.000000e+00\n",
      "3       boss   958.186602  2.203870e-210\n",
      "4     lawyer    18.934758   1.352656e-05\n",
      "5  paralegal  1112.738997  5.625368e-244\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             1.20             0.59             0.53\n",
      "1      nurse             1.20             0.80             0.69\n",
      "2  secretary             1.20             0.74             0.92\n",
      "3       boss             3.51             3.51             3.51\n",
      "4     lawyer             1.20             1.90             2.30\n",
      "5  paralegal             1.20             0.88             0.92\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.39        0.19        0.16\n",
      "1      nurse       -0.91       -0.71       -0.58\n",
      "2  secretary       -1.00       -0.71       -0.88\n",
      "3       boss        0.69        0.69        0.66\n",
      "4     lawyer        0.59        0.54        0.56\n",
      "5  paralegal       -1.00       -0.86       -0.88\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>ddp_top_10</th>\n",
       "      <th>ddp_top_20</th>\n",
       "      <th>ddp_top_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doctor</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nurse</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>secretary</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boss</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lawyer</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>paralegal</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>-0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
       "0     doctor        0.39        0.19        0.16\n",
       "1      nurse       -0.91       -0.71       -0.58\n",
       "2  secretary       -1.00       -0.71       -0.88\n",
       "3       boss        0.69        0.69        0.66\n",
       "4     lawyer        0.59        0.54        0.56\n",
       "5  paralegal       -1.00       -0.86       -0.88"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = open_clip.get_tokenizer('ViT-B-16')\n",
    "num_gender_queries = 5\n",
    "\n",
    "\n",
    "queries = [ \"doctor\", \"nurse\", \"secretary\", 'boss', 'lawyer', 'paralegal']\n",
    "\n",
    "text_inputs = torch.cat([tokenizer(f\"This is a photo of a {word}\") for word in queries]).to(device)\n",
    "# text_inputs = clip.tokenize([desc for desc in queries]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)#.float()\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "similarity = (100.0 * all_features_test @ text_features.T).cpu().numpy().astype(np.float64).T\n",
    "\n",
    "ut.calc_similarity_diff('orig_flickr_open','gender', queries, protected_attribute, {0: 'Female', 1:'Male'}, similarity)\n",
    "ut.run_anova(queries, all_labels_gender_test, similarity, 'orig_flickr_open', skip_att = 2)\n",
    "ut.run_skew(queries, all_labels_gender_test, similarity, 'orig_flickr_open', [10, 20, 30], skip_attr = 2)\n",
    "ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, 'orig_flickr_open', [10, 20, 30], skip_attr = 2)\n",
    "# ut.run_relevance(queries, queries_tokens, all_labels_gender_test,similarity/_test, all_labels_captions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa2fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_tasks = {}\n",
    "classification_tasks['gender'] = [\n",
    "    ('criminal','innocent person'),\n",
    "    ('nurse','doctor'),\n",
    "    ('prostitute','night owl'),\n",
    "    ('CEO','secretary'),\n",
    "    ('leader','follower'),\n",
    "    ('lawyer','paralegal'),\n",
    "    ('secretary','board member'),\n",
    "    ('convict',\t'exoneree'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ed3bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.41  0.71       0.30\n",
      "(nurse, doctor)                0.79  0.29      -0.50\n",
      "(prostitute, night owl)        0.64  0.39      -0.25\n",
      "(CEO, secretary)               0.21  0.79       0.58\n",
      "(leader, follower)             0.43  0.60       0.17\n",
      "(lawyer, paralegal)            0.31  0.80       0.49\n",
      "(secretary, board member)      0.49  0.14      -0.35\n",
      "(convict, exoneree)            0.10  0.24       0.14\n"
     ]
    }
   ],
   "source": [
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([tokenizer(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * all_features_test @ text_features.T).softmax(dim=-1).cpu().numpy().astype(np.float64)\n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        for ell in range(2):\n",
    "#             print(ell)\n",
    "            temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_flickr_clf_orig_open.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca79ae4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error of predicting gender train = 0.37\n"
     ]
    }
   ],
   "source": [
    "projection_GT,projection_inferred, MI_GT, MI_inferred = ut.calculate_projections_flickr(all_features_train, all_labels_gender_train, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a8d9287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running Fair pca G.T on the model ============== \n",
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female   Male  Disparity\n",
      "doctor      12.33  12.40       0.07\n",
      "nurse       12.33  12.29      -0.04\n",
      "secretary   12.38  12.40       0.02\n",
      "boss        17.02  16.97      -0.05\n",
      "lawyer      12.49  12.58       0.09\n",
      "paralegal   12.00  12.01       0.01\n",
      "-------------------------------------------------------------------\n",
      "       Query      stat      pval\n",
      "0     doctor  0.722227  0.395414\n",
      "1      nurse  0.257399  0.611913\n",
      "2  secretary  0.037750  0.845946\n",
      "3       boss  0.825561  0.363559\n",
      "4     lawyer  1.163798  0.280679\n",
      "5  paralegal  0.012953  0.909388\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             0.59             0.30             0.36\n",
      "1      nurse             0.18             0.29             0.11\n",
      "2  secretary             0.59             0.59             0.53\n",
      "3       boss             0.59             0.59             0.59\n",
      "4     lawyer             1.20             0.80             1.20\n",
      "5  paralegal             0.41             0.41             0.47\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.19       -0.06        0.02\n",
      "1      nurse       -0.31       -0.06       -0.15\n",
      "2  secretary       -0.01       -0.41       -0.08\n",
      "3       boss        0.19        0.09        0.16\n",
      "4     lawyer        0.39        0.24        0.39\n",
      "5  paralegal       -0.21       -0.26       -0.08\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running Fair pca G.T on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    \n",
    "    text_inputs = torch.cat([tokenizer(f\"a photo of a {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    projection_train = projection_GT[attr]\n",
    "    all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "    text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "    similarity = (100.0 * all_features_val_transf @ text_features_pca.T).T\n",
    "    ut.calc_similarity_diff('fpca_gt_flickr_open','gender', queries, protected_attribute, {0: 'Female', 1:'Male'}, similarity)\n",
    "    ut.run_anova(queries, all_labels_gender_test, similarity, 'fpca_gt_flickr_open', skip_att = 2)\n",
    "    ut.run_skew(queries, all_labels_gender_test, similarity, 'fpca_gt_flickr_open', [10, 20, 30], skip_attr = 2)\n",
    "    ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, 'fpca_gt_flickr_open', [10, 20, 30], skip_attr = 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a72a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running CLF Fair pca G.T on the model ============== \n",
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.59  0.59       0.00\n",
      "(nurse, doctor)                0.49  0.49       0.00\n",
      "(prostitute, night owl)        0.50  0.51       0.01\n",
      "(CEO, secretary)               0.55  0.54      -0.01\n",
      "(leader, follower)             0.54  0.52      -0.02\n",
      "(lawyer, paralegal)            0.60  0.60       0.00\n",
      "(secretary, board member)      0.26  0.25      -0.01\n",
      "(convict, exoneree)            0.16  0.18       0.02\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running CLF Fair pca G.T on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([tokenizer(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        projection_train = projection_GT[attr]\n",
    "        all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "        text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "        similarity = softmax(100.0 * np.matmul(all_features_val_transf, np.transpose(text_features_pca)),axis=1)\n",
    "        \n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        for ell in range(2):\n",
    "#             print(ell)\n",
    "            temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_flickr_clf_fpca_gt_open.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ebec9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running Fair pca INF on the model ============== \n",
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female   Male  Disparity\n",
      "doctor      12.51  12.31      -0.20\n",
      "nurse       12.26  12.08      -0.18\n",
      "secretary   12.46  12.08      -0.38\n",
      "boss        17.34  16.94      -0.40\n",
      "lawyer      12.67  12.50      -0.17\n",
      "paralegal   12.12  11.75      -0.37\n",
      "-------------------------------------------------------------------\n",
      "       Query       stat          pval\n",
      "0     doctor   5.506409  1.894691e-02\n",
      "1      nurse   5.941749  1.478634e-02\n",
      "2  secretary  19.800990  8.593815e-06\n",
      "3       boss  52.617540  4.052538e-13\n",
      "4     lawyer   4.443166  3.504121e-02\n",
      "5  paralegal  21.835326  2.970798e-06\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             0.59             0.30             0.36\n",
      "1      nurse             0.18             0.18             0.10\n",
      "2  secretary             0.59             0.59             0.41\n",
      "3       boss             0.41             0.50             0.59\n",
      "4     lawyer             1.20             0.80             0.59\n",
      "5  paralegal             0.59             0.41             0.34\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.19       -0.06        0.02\n",
      "1      nurse       -0.31       -0.11       -0.21\n",
      "2  secretary       -0.01       -0.41       -0.21\n",
      "3       boss       -0.11       -0.01        0.12\n",
      "4     lawyer        0.29        0.24        0.19\n",
      "5  paralegal       -0.41       -0.26       -0.21\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running Fair pca INF on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    \n",
    "    text_inputs = torch.cat([tokenizer(f\"a photo of a {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    projection_train = projection_inferred[attr]\n",
    "    all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "    text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "    similarity = (100.0 * all_features_val_transf @ text_features_pca.T).T\n",
    "    ut.calc_similarity_diff('fpca_inf_flickr_open','gender', queries, protected_attribute, {0: 'Female', 1:'Male'}, similarity)\n",
    "    ut.run_anova(queries, all_labels_gender_test, similarity, 'fpca_inf_flickr_open', skip_att = 2)\n",
    "    ut.run_skew(queries, all_labels_gender_test, similarity, 'fpca_inf_flickr_open', [10, 20, 30], skip_attr = 2)\n",
    "    ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, 'fpca_inf_flickr_open', [10, 20, 30], skip_attr = 2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07ed110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running CLF Fair pca inf on the model ============== \n",
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.61  0.61       0.00\n",
      "(nurse, doctor)                0.46  0.48       0.02\n",
      "(prostitute, night owl)        0.50  0.48      -0.02\n",
      "(CEO, secretary)               0.58  0.57      -0.01\n",
      "(leader, follower)             0.54  0.54       0.00\n",
      "(lawyer, paralegal)            0.61  0.63       0.02\n",
      "(secretary, board member)      0.25  0.24      -0.01\n",
      "(convict, exoneree)            0.16  0.19       0.03\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running CLF Fair pca inf on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([tokenizer(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        projection_train = projection_inferred[attr]\n",
    "        all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "        text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "        similarity = softmax(100.0 * np.matmul(all_features_val_transf, np.transpose(text_features_pca)),axis=1)\n",
    "        \n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        for ell in range(2):\n",
    "#             print(ell)\n",
    "            temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_flickr_clf_fpca_inf_open.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4d0a46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running MI G.T on the model ============== \n",
      "..... 400.........\n",
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female   Male  Disparity\n",
      "doctor       8.11   8.22       0.11\n",
      "nurse        7.94   7.82      -0.12\n",
      "secretary    8.73   8.09      -0.64\n",
      "boss        13.22  14.01       0.79\n",
      "lawyer       8.41   8.72       0.31\n",
      "paralegal    8.00   7.96      -0.04\n",
      "-------------------------------------------------------------------\n",
      "       Query        stat          pval\n",
      "0     doctor    2.763873  9.641458e-02\n",
      "1      nurse    4.351256  3.698167e-02\n",
      "2  secretary  104.671804  1.441376e-24\n",
      "3       boss  280.476589  5.912069e-63\n",
      "4     lawyer   24.988300  5.767927e-07\n",
      "5  paralegal    0.263535  6.077021e-01\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             3.51             1.20             0.92\n",
      "1      nurse             0.41             0.30             0.34\n",
      "2  secretary             0.59             0.50             0.59\n",
      "3       boss             3.51             1.90             1.61\n",
      "4     lawyer             0.51             1.20             1.61\n",
      "5  paralegal             0.41             0.41             0.26\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.49        0.24        0.22\n",
      "1      nurse       -0.01       -0.11       -0.05\n",
      "2  secretary       -0.41       -0.36       -0.48\n",
      "3       boss        0.69        0.59        0.56\n",
      "4     lawyer        0.09        0.39        0.46\n",
      "5  paralegal       -0.41       -0.06       -0.15\n",
      "..... 256.........\n",
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female   Male  Disparity\n",
      "doctor       7.18   7.48       0.30\n",
      "nurse        7.33   7.42       0.09\n",
      "secretary    7.57   7.50      -0.07\n",
      "boss        12.81  13.11       0.30\n",
      "lawyer       7.26   7.59       0.33\n",
      "paralegal    6.71   6.71       0.00\n",
      "-------------------------------------------------------------------\n",
      "       Query       stat          pval\n",
      "0     doctor  67.338956  2.286191e-16\n",
      "1      nurse   5.100598  2.391760e-02\n",
      "2  secretary   2.849522  9.140112e-02\n",
      "3       boss  63.821397  1.362259e-15\n",
      "4     lawyer  59.898199  9.989281e-15\n",
      "5  paralegal   0.006612  9.351902e-01\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             3.51             1.90             0.51\n",
      "1      nurse             0.18             0.30             0.18\n",
      "2  secretary             3.51             0.67             0.47\n",
      "3       boss             1.20             1.90             2.30\n",
      "4     lawyer             0.51             0.51             0.51\n",
      "5  paralegal             0.41             0.30             0.34\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.29        0.24        0.05\n",
      "1      nurse       -0.31       -0.06       -0.15\n",
      "2  secretary       -1.00       -0.66       -0.41\n",
      "3       boss        0.19        0.49        0.46\n",
      "4     lawyer       -0.01        0.14        0.09\n",
      "5  paralegal       -0.21       -0.26       -0.25\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running MI G.T on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    \n",
    "    text_inputs = torch.cat([tokenizer(f\"a photo of a {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features.cpu().numpy().astype(np.float64)\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_GT[attr]\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"..... {num_clip}.........\")\n",
    "        \n",
    "        text_features_mi =text_features[:, mis[:num_clip]]\n",
    "        image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "        similarity = (100.0 * image_features_val @ text_features_mi.T).T \n",
    "        ut.calc_similarity_diff(f'MI_gt{num_clip}_flickr_open','gender', queries, protected_attribute, {0: 'Female', 1:'Male'}, similarity)\n",
    "        ut.run_anova(queries, all_labels_gender_test, similarity, f'MI_gt{num_clip}_flickr_open', skip_att = 2)\n",
    "        ut.run_skew(queries, all_labels_gender_test, similarity, f'MI_gt{num_clip}_flickr_open', [10, 20, 30], skip_attr = 2)\n",
    "        ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, f'MI_gt{num_clip}_flickr_open', [10, 20, 30], skip_attr = 2)\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afb92199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running MI INF on the model ============== \n",
      "..... 400.........\n",
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female   Male  Disparity\n",
      "doctor       8.00   8.19       0.19\n",
      "nurse        7.68   7.39      -0.29\n",
      "secretary    8.40   7.87      -0.53\n",
      "boss        13.63  13.86       0.23\n",
      "lawyer       7.84   8.27       0.43\n",
      "paralegal    7.16   6.95      -0.21\n",
      "-------------------------------------------------------------------\n",
      "       Query       stat          pval\n",
      "0     doctor  10.877146  9.735817e-04\n",
      "1      nurse  29.081639  6.939152e-08\n",
      "2  secretary  80.913961  2.357622e-19\n",
      "3       boss  19.188633  1.184165e-05\n",
      "4     lawyer  61.633523  4.137147e-15\n",
      "5  paralegal  13.225535  2.761609e-04\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             3.51             0.80             0.34\n",
      "1      nurse             0.41             0.18             0.22\n",
      "2  secretary             0.74             0.81             0.59\n",
      "3       boss             3.51             0.59             0.64\n",
      "4     lawyer             0.18             0.51             0.53\n",
      "5  paralegal             1.20             0.67             0.41\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.49        0.14       -0.01\n",
      "1      nurse       -0.01       -0.16       -0.08\n",
      "2  secretary       -0.61       -0.71       -0.48\n",
      "3       boss        0.59        0.14        0.16\n",
      "4     lawyer       -0.11        0.09        0.16\n",
      "5  paralegal       -0.81       -0.66       -0.31\n",
      "..... 256.........\n",
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female  Male  Disparity\n",
      "doctor       5.78  5.87       0.09\n",
      "nurse        4.99  5.03       0.04\n",
      "secretary    5.33  5.14      -0.19\n",
      "boss         6.55  6.58       0.03\n",
      "lawyer       5.58  5.70       0.12\n",
      "paralegal    4.92  4.77      -0.15\n",
      "-------------------------------------------------------------------\n",
      "       Query       stat      pval\n",
      "0     doctor   5.651140  0.017444\n",
      "1      nurse   1.046162  0.306393\n",
      "2  secretary  18.609499  0.000016\n",
      "3       boss   0.791170  0.373747\n",
      "4     lawyer   7.847785  0.005088\n",
      "5  paralegal  12.007503  0.000530\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             0.51             0.41             0.34\n",
      "1      nurse             0.59             0.30             0.34\n",
      "2  secretary             1.20             0.59             0.47\n",
      "3       boss             0.74             0.41             0.26\n",
      "4     lawyer             0.41             0.30             0.47\n",
      "5  paralegal             0.41             0.50             0.41\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.09        0.04       -0.11\n",
      "1      nurse       -0.61       -0.21       -0.35\n",
      "2  secretary       -0.91       -0.51       -0.38\n",
      "3       boss        0.29        0.04       -0.08\n",
      "4     lawyer       -0.01       -0.01        0.05\n",
      "5  paralegal       -0.11       -0.41       -0.31\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running MI INF on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    \n",
    "    text_inputs = torch.cat([tokenizer(f\"a photo of a {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features.cpu().numpy().astype(np.float64)\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_inferred[attr]\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"..... {num_clip}.........\")\n",
    "        \n",
    "        text_features_mi =text_features[:, mis[:num_clip]]\n",
    "        image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "        similarity = (100.0 * image_features_val @ text_features_mi.T).T \n",
    "        ut.calc_similarity_diff(f'MI_inf{num_clip}_flickr_open','gender', queries, protected_attribute, {0: 'Female', 1:'Male'}, similarity)\n",
    "        ut.run_anova(queries, all_labels_gender_test, similarity, f'MI_inf{num_clip}_flickr_open', skip_att = 2)\n",
    "        ut.run_skew(queries, all_labels_gender_test, similarity, f'MI_inf{num_clip}_flickr_open', [10, 20, 30], skip_attr = 2)\n",
    "        ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, f'MI_inf{num_clip}_flickr_open', [10, 20, 30], skip_attr = 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cf0ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running CLF MI G.T on the model ============== \n",
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "----------- 400--------------\n",
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.81  0.85       0.04\n",
      "(nurse, doctor)                0.54  0.50      -0.04\n",
      "(prostitute, night owl)        0.42  0.37      -0.05\n",
      "(CEO, secretary)               0.36  0.54       0.18\n",
      "(leader, follower)             0.68  0.73       0.05\n",
      "(lawyer, paralegal)            0.52  0.59       0.07\n",
      "(secretary, board member)      0.32  0.20      -0.12\n",
      "(convict, exoneree)            0.18  0.28       0.10\n",
      "----------- 256--------------\n",
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.86  0.90       0.04\n",
      "(nurse, doctor)                0.64  0.58      -0.06\n",
      "(prostitute, night owl)        0.25  0.22      -0.03\n",
      "(CEO, secretary)               0.30  0.31       0.01\n",
      "(leader, follower)             0.79  0.83       0.04\n",
      "(lawyer, paralegal)            0.59  0.67       0.08\n",
      "(secretary, board member)      0.42  0.33      -0.09\n",
      "(convict, exoneree)            0.10  0.14       0.04\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running CLF MI G.T on the model ============== \")\n",
    "\n",
    "for attr in ['gender']:\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_GT[attr]\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"----------- {num_clip}--------------\")\n",
    "        temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "        for cc, task in enumerate(classification_tasks[attr]):\n",
    "            text_inputs = torch.cat([tokenizer(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_inputs)\n",
    "            text_features_mi =text_features.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            similarity = softmax(100.0 * np.matmul(image_features_val, np.transpose(text_features_mi)),axis=1)\n",
    "\n",
    "            predictions = np.argmax(similarity,axis=1)\n",
    "            for ell in range(2):\n",
    "    #             print(ell)\n",
    "                temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "        columns= ['Female', 'Male']\n",
    "        temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "        if attr == 'gender':\t  \n",
    "            temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "        elif attr == 'race':\n",
    "            temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "        temp.to_csv(f\"../results_csv/{attr}_flickr_clf_MI_gt{num_clip}_open.csv\")\n",
    "        print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767b3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08fe8dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running CLF MI inf on the model ============== \n",
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "----------- 400--------------\n",
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.64  0.71       0.07\n",
      "(nurse, doctor)                0.51  0.42      -0.09\n",
      "(prostitute, night owl)        0.39  0.36      -0.03\n",
      "(CEO, secretary)               0.36  0.47       0.11\n",
      "(leader, follower)             0.71  0.77       0.06\n",
      "(lawyer, paralegal)            0.60  0.73       0.13\n",
      "(secretary, board member)      0.35  0.27      -0.08\n",
      "(convict, exoneree)            0.09  0.14       0.05\n",
      "----------- 256--------------\n",
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.61  0.60      -0.01\n",
      "(nurse, doctor)                0.35  0.34      -0.01\n",
      "(prostitute, night owl)        0.43  0.39      -0.04\n",
      "(CEO, secretary)               0.45  0.48       0.03\n",
      "(leader, follower)             0.77  0.80       0.03\n",
      "(lawyer, paralegal)            0.63  0.72       0.09\n",
      "(secretary, board member)      0.37  0.35      -0.02\n",
      "(convict, exoneree)            0.54  0.55       0.01\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running CLF MI inf on the model ============== \")\n",
    "\n",
    "for attr in ['gender']:\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_inferred[attr]\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"----------- {num_clip}--------------\")\n",
    "        temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "        for cc, task in enumerate(classification_tasks[attr]):\n",
    "            text_inputs = torch.cat([tokenizer(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_inputs)\n",
    "            text_features_mi =text_features.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            similarity = softmax(100.0 * np.matmul(image_features_val, np.transpose(text_features_mi)),axis=1)\n",
    "\n",
    "            predictions = np.argmax(similarity,axis=1)\n",
    "            for ell in range(2):\n",
    "    #             print(ell)\n",
    "                temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "        columns= ['Female', 'Male']\n",
    "        temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "        if attr == 'gender': \n",
    "            temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "        elif attr == 'race':\n",
    "            temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "        temp.to_csv(f\"../results_csv/{attr}_flickr_clf_MI_inf{num_clip}_open.csv\")\n",
    "        print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004dda2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af0bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
