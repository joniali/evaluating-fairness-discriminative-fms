{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2c5c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "from PIL import Image\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "# from IPython.display import Image, display\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "clip.available_models()\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import flickr_dataset as fl \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "from scipy import stats\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import utils as ut\n",
    "import importlib\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ac546b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/mnt/efs/fairclip/FinalCode/Flickr30K/../utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8a32a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afbda386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" #if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac67fa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 0\n",
      "15507 15507\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(fl)\n",
    "flickrData = fl.MyFlickr30k('../../flicker30k-images/flickr30k-images', '../../flicker30k-captions/results_20130124.token',None, transform=preprocess)\n",
    "train_size = int(0.5 * len(flickrData))\n",
    "test_size = len(flickrData) - train_size\n",
    "torch.manual_seed(0)\n",
    "set_seed(0)\n",
    "flickrData_train, flickrData_test = torch.utils.data.random_split(flickrData, [train_size, test_size])\n",
    "print(len(flickrData_test), len(flickrData_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419f00f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15507 15507\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(fl)\n",
    "flickrData_orig = fl.MyFlickr30k('../../flicker30k-images/flickr30k-images', '../../flicker30k-captions/results_20130124.token',None)\n",
    "train_size = int(0.5 * len(flickrData))\n",
    "test_size = len(flickrData_orig) - train_size\n",
    "torch.manual_seed(0)\n",
    "flickrData_train_orig, flickrData_test_orig = torch.utils.data.random_split(flickrData_orig, [train_size, test_size])\n",
    "print(len(flickrData_test_orig), len(flickrData_train_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21dc58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "077858f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [01:53<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "all_features_train, all_labels_captions_train, all_labels_gender_train = ut.get_features_flickr(flickrData_train, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02902e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [01:52<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "all_features_test, all_labels_captions_test, all_labels_gender_test = ut.get_features_flickr(flickrData_test, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ab555785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Karpathy We do not use the Karpathy split but we provide the code. Just uncomment the following lines\n",
    "# and change the variable all_features_test to all_features_test_k \n",
    "# importlib.reload(fl)\n",
    "# flickrData = fl.MyFlickr30k('flicker30k-images/flickr30k-images', 'flicker30k-captions/results_20130124.token','test', transform=preprocess)\n",
    "# all_features_test_k, all_labels_captions_test_k, all_labels_gender_test_k = ut.get_features_flickr(flickrData, model, device)\n",
    "# all_features_test_k /= all_features_test_k.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be84773",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_test /= all_features_test.norm(dim=-1, keepdim=True)\n",
    "all_features_train /= all_features_train.norm(dim=-1, keepdim=True)\n",
    "protected_attribute = {'gender':all_labels_gender_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db79041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female   Male  Disparity\n",
      "doctor      19.60  20.17       0.57\n",
      "nurse       21.11  19.62      -1.49\n",
      "secretary   20.58  18.87      -1.71\n",
      "boss        19.72  20.55       0.83\n",
      "lawyer      19.58  20.15       0.57\n",
      "paralegal   21.53  20.61      -0.92\n",
      "-------------------------------------------------------------------\n",
      "       Query         stat           pval\n",
      "0     doctor   271.013913   6.822332e-61\n",
      "1      nurse  1251.572236  3.779270e-274\n",
      "2  secretary  1567.179037   0.000000e+00\n",
      "3       boss   587.675394  8.026846e-130\n",
      "4     lawyer   218.466081   1.954230e-49\n",
      "5  paralegal   521.562709  1.934438e-115\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             3.51             1.90             1.61\n",
      "1      nurse             1.20             1.90             1.61\n",
      "2  secretary             3.51             0.88             0.83\n",
      "3       boss             1.20             1.90             2.30\n",
      "4     lawyer             3.51             3.51             2.30\n",
      "5  paralegal             0.74             0.50             0.47\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.39        0.29        0.36\n",
      "1      nurse       -0.91       -0.96       -0.91\n",
      "2  secretary       -1.00       -0.86       -0.81\n",
      "3       boss        0.49        0.64        0.69\n",
      "4     lawyer        0.79        0.74        0.69\n",
      "5  paralegal       -0.61       -0.41       -0.51\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>ddp_top_10</th>\n",
       "      <th>ddp_top_20</th>\n",
       "      <th>ddp_top_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doctor</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nurse</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>secretary</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>-0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boss</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lawyer</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>paralegal</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
       "0     doctor        0.39        0.29        0.36\n",
       "1      nurse       -0.91       -0.96       -0.91\n",
       "2  secretary       -1.00       -0.86       -0.81\n",
       "3       boss        0.49        0.64        0.69\n",
       "4     lawyer        0.79        0.74        0.69\n",
       "5  paralegal       -0.61       -0.41       -0.51"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_gender_queries = 5\n",
    "\n",
    "queries = [ \"doctor\", \"nurse\", \"secretary\", 'boss', 'lawyer', 'paralegal']\n",
    "\n",
    "text_inputs = torch.cat([clip.tokenize(f\"This is a photo of a {word}\") for word in queries]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)#.float()\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "similarity = (100.0 * all_features_test @ text_features.T).cpu().numpy().astype(np.float64).T\n",
    "ut.calc_similarity_diff('orig_flickr', 'gender', queries, protected_attribute , {0: 'Female', 1:'Male'},similarity)\n",
    "ut.run_anova(queries, all_labels_gender_test, similarity, 'orig_flickr', skip_att = 2)\n",
    "ut.run_skew(queries, all_labels_gender_test, similarity, 'orig_flickr', [10, 20, 30], skip_attr = 2)\n",
    "ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, 'orig_flickr', [10, 20, 30], skip_attr = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1170c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b81985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall test Karpathy\n",
    "# import time\n",
    "# start = time.time()\n",
    "# flat_captions = all_labels_captions_test_k.flatten()\n",
    "# similarity = []\n",
    "# for cap in flat_captions:\n",
    "#     #cap[:300]\n",
    "#     text_inputs = clip.tokenize(f\"a photo of {cap}\", truncate = True).to(device)\n",
    "# # for cap in all_labels_captions_test_k: #flat_captions:\n",
    "    \n",
    "# #     text_inputs = clip.tokenize(f\"a photo of {''.join(cap)}\", truncate = True).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         text_features = model.encode_text(text_inputs)#.float()\n",
    "#     text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#     similarity.append((100.0 * all_features_test_k @ text_features.T).cpu().numpy().astype(np.float64))\n",
    "# similarity = np.asarray(similarity).squeeze()\n",
    "# print(similarity.shape)\n",
    "# # similarity = np.asarray(similarity).reshape((77535,15507))\n",
    "# ut.calculate_recall(similarity, \"orig_flickr_karp\")\n",
    "# end = time.time()\n",
    "# print(end - start)\n",
    "# using all the descriptions joined \n",
    "# mean_top_1  mean_top_5  mean_top_10\n",
    "# 0        0.79        0.97         0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3de8fdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77535, 15507)\n",
      "   mean_top_1  mean_top_5  mean_top_10\n",
      "0       0.315       0.548        0.647\n",
      "667.9809448719025\n"
     ]
    }
   ],
   "source": [
    "#recall \n",
    "importlib.reload(ut)\n",
    "import time\n",
    "start = time.time()\n",
    "flat_captions = all_labels_captions_test.flatten()\n",
    "similarity = []\n",
    "for cap in flat_captions:\n",
    "    #cap[:300]\n",
    "    text_inputs = clip.tokenize(f\"a photo of {cap}\", truncate = True).to(device)\n",
    "# for cap in all_labels_captions_test: #flat_captions:\n",
    "    \n",
    "#     text_inputs = clip.tokenize(f\"a photo of {''.join(cap)}\", truncate = True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)#.float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity.append((100.0 * all_features_test @ text_features.T).cpu().numpy().astype(np.float64))\n",
    "similarity = np.asarray(similarity).squeeze()\n",
    "print(similarity.shape)\n",
    "# similarity = np.asarray(similarity).reshape((77535,15507 ))\n",
    "ut.calculate_recall(similarity, \"orig_flickr\")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5a865e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77535,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.315       0.548        0.647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baa2fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_tasks = {}\n",
    "classification_tasks['gender'] = [\n",
    "    ('criminal','innocent person'),\n",
    "    ('nurse','doctor'),\n",
    "    ('prostitute','night owl'),\n",
    "    ('CEO','secretary'),\n",
    "    ('leader','follower'),\n",
    "    ('lawyer','paralegal'),\n",
    "    ('secretary','board member'),\n",
    "    ('convict',\t'exoneree'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2ed3bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.05  0.13       0.08\n",
      "(nurse, doctor)                0.80  0.24      -0.56\n",
      "(prostitute, night owl)        0.73  0.51      -0.22\n",
      "(CEO, secretary)               0.25  0.80       0.55\n",
      "(leader, follower)             0.08  0.17       0.09\n",
      "(lawyer, paralegal)            0.08  0.40       0.32\n",
      "(secretary, board member)      0.17  0.02      -0.15\n",
      "(convict, exoneree)            0.28  0.14      -0.14\n"
     ]
    }
   ],
   "source": [
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * all_features_test @ text_features.T).softmax(dim=-1).cpu().numpy().astype(np.float64)\n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        for ell in range(2):\n",
    "#             print(ell)\n",
    "            temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_flickr_clf_orig.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca79ae4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error of predicting gender train = 0.35\n"
     ]
    }
   ],
   "source": [
    "projection_GT,projection_inferred, MI_GT, MI_inferred = ut.calculate_projections_flickr(all_features_train, all_labels_gender_train, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10468a",
   "metadata": {},
   "source": [
    "# Fair PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a8d9287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running Fair pca G.T on the model ============== \n",
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female   Male  Disparity\n",
      "doctor      20.64  20.68       0.04\n",
      "nurse       20.14  20.11      -0.03\n",
      "secretary   19.57  19.54      -0.03\n",
      "boss        20.58  20.57      -0.01\n",
      "lawyer      20.36  20.36       0.00\n",
      "paralegal   20.91  20.92       0.01\n",
      "-------------------------------------------------------------------\n",
      "       Query      stat      pval\n",
      "0     doctor  1.471132  0.225167\n",
      "1      nurse  0.496238  0.481158\n",
      "2  secretary  0.489518  0.484143\n",
      "3       boss  0.082629  0.773766\n",
      "4     lawyer  0.007272  0.932042\n",
      "5  paralegal  0.085304  0.770234\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             3.51             0.80             0.51\n",
      "1      nurse             0.59             0.41             0.34\n",
      "2  secretary             0.74             0.74             0.74\n",
      "3       boss             0.74             0.74             0.83\n",
      "4     lawyer             1.20             1.20             1.20\n",
      "5  paralegal             0.74             0.41             0.34\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.39        0.04        0.05\n",
      "1      nurse       -0.51       -0.41       -0.35\n",
      "2  secretary        0.19        0.24        0.26\n",
      "3       boss        0.19        0.24        0.39\n",
      "4     lawyer        0.59        0.49        0.46\n",
      "5  paralegal        0.19       -0.06       -0.08\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running Fair pca G.T on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    projection_train = projection_GT[attr]\n",
    "    all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "    text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "    similarity = (100.0 * all_features_val_transf @ text_features_pca.T).T\n",
    "    ut.calc_similarity_diff('fpca_gt_flickr', 'gender',queries, protected_attribute , {0: 'Female', 1:'Male'},similarity)\n",
    "    ut.run_anova(queries, all_labels_gender_test, similarity, 'fpca_gt_flickr', skip_att = 2)\n",
    "    ut.run_skew(queries, all_labels_gender_test, similarity, 'fpca_gt_flickr', [10, 20, 30],skip_attr = 2)\n",
    "    ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, 'fpca_gt_flickr',[10, 20, 30], skip_attr = 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8a72a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running CLF Fair pca G.T on the model ============== \n",
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.10  0.07      -0.03\n",
      "(nurse, doctor)                0.36  0.34      -0.02\n",
      "(prostitute, night owl)        0.57  0.57       0.00\n",
      "(CEO, secretary)               0.67  0.68       0.01\n",
      "(leader, follower)             0.15  0.14      -0.01\n",
      "(lawyer, paralegal)            0.30  0.31       0.01\n",
      "(secretary, board member)      0.04  0.03      -0.01\n",
      "(convict, exoneree)            0.17  0.16      -0.01\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running CLF Fair pca G.T on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        projection_train = projection_GT[attr]\n",
    "        all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "        text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "        similarity = softmax(100.0 * np.matmul(all_features_val_transf, np.transpose(text_features_pca)),axis=1)\n",
    "        \n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        for ell in range(2):\n",
    "#             print(ell)\n",
    "            temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_flickr_clf_fpca_gt.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "253c908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running Recall Fair pca with G.T attribute ============== \n",
      "   mean_top_1  mean_top_5  mean_top_10\n",
      "0       0.289       0.513        0.611\n",
      "4389.975239038467\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running Recall Fair pca with G.T attribute ============== \")\n",
    "import time\n",
    "start = time.time()\n",
    "flat_captions = all_labels_captions_test.flatten()\n",
    "similarity = []\n",
    "projection_train = projection_GT['gender']\n",
    "all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "\n",
    "for cap in flat_captions:\n",
    "    text_inputs = clip.tokenize(f\"a photo of {cap}\", truncate = True).to(device)\n",
    "# for cap in all_labels_captions_test: #flat_captions:\n",
    "    \n",
    "#     text_inputs = clip.tokenize(f\"a photo of {''.join(cap)}\", truncate = True).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)#.float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "    similarity.append(100.0 * all_features_val_transf @ text_features_pca.T)\n",
    "similarity = np.asarray(similarity).squeeze()#reshape((77535,15507 ))\n",
    "ut.calculate_recall(similarity, \"fpca_gt_flickr\")\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ebec9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running Fair pca INF on the model ============== \n",
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female   Male  Disparity\n",
      "doctor      20.80  20.68      -0.12\n",
      "nurse       20.08  20.02      -0.06\n",
      "secretary   19.73  19.42      -0.31\n",
      "boss        20.84  20.57      -0.27\n",
      "lawyer      20.50  20.36      -0.14\n",
      "paralegal   20.99  20.85      -0.14\n",
      "-------------------------------------------------------------------\n",
      "       Query       stat          pval\n",
      "0     doctor  11.636906  6.465607e-04\n",
      "1      nurse   1.745813  1.864039e-01\n",
      "2  secretary  59.017802  1.562520e-14\n",
      "3       boss  65.217821  6.706033e-16\n",
      "4     lawyer  13.158204  2.862632e-04\n",
      "5  paralegal  14.703837  1.257902e-04\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             3.51             0.80             0.69\n",
      "1      nurse             0.74             0.67             0.34\n",
      "2  secretary             0.59             0.59             0.47\n",
      "3       boss             0.41             0.59             0.69\n",
      "4     lawyer             1.20             0.74             0.74\n",
      "5  paralegal             0.74             0.18             0.26\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.39        0.04        0.09\n",
      "1      nurse       -0.71       -0.61       -0.28\n",
      "2  secretary       -0.01        0.04       -0.11\n",
      "3       boss       -0.21        0.09        0.22\n",
      "4     lawyer        0.59        0.29        0.32\n",
      "5  paralegal        0.19       -0.21       -0.15\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running Fair pca INF on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    \n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    projection_train = projection_inferred[attr]\n",
    "    all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "    text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "    similarity = (100.0 * all_features_val_transf @ text_features_pca.T).T\n",
    "    ut.calc_similarity_diff('fpca_inf_flickr', 'gender', queries, protected_attribute , {0: 'Female', 1:'Male'},similarity)\n",
    "    ut.run_anova(queries, all_labels_gender_test, similarity, 'fpca_inf_flickr', skip_att = 2)\n",
    "    ut.run_skew(queries, all_labels_gender_test, similarity, 'fpca_inf_flickr',[10, 20, 30], skip_attr = 2)\n",
    "    ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, 'fpca_inf_flickr',[10, 20, 30], skip_attr = 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07ed110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running CLF Fair pca inf on the model ============== \n",
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.11  0.06      -0.05\n",
      "(nurse, doctor)                0.29  0.31       0.02\n",
      "(prostitute, night owl)        0.54  0.57       0.03\n",
      "(CEO, secretary)               0.70  0.72       0.02\n",
      "(leader, follower)             0.17  0.14      -0.03\n",
      "(lawyer, paralegal)            0.33  0.34       0.01\n",
      "(secretary, board member)      0.03  0.02      -0.01\n",
      "(convict, exoneree)            0.18  0.15      -0.03\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running CLF Fair pca inf on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        projection_train = projection_inferred[attr]\n",
    "        all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "        text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "        similarity = softmax(100.0 * np.matmul(all_features_val_transf, np.transpose(text_features_pca)),axis=1)\n",
    "        \n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        for ell in range(2):\n",
    "#             print(ell)\n",
    "            temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_flickr_clf_fpca_inf.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e917e766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running Recall pca Inferred on the model ============== \n",
      "   mean_top_1  mean_top_5  mean_top_10\n",
      "0       0.288       0.513        0.612\n",
      "3530.787236213684\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running Recall pca Inferred on the model ============== \")\n",
    "import time\n",
    "start = time.time()\n",
    "flat_captions = all_labels_captions_test.flatten()\n",
    "similarity = []\n",
    "projection_train = projection_inferred['gender']\n",
    "all_features_val_transf = projection_train.just_transform(all_features_test.cpu().numpy().astype(np.float64))\n",
    "\n",
    "for cap in flat_captions:\n",
    "    text_inputs = clip.tokenize(f\"a photo of {cap}\", truncate = True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)#.float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "    similarity.append(100.0 * all_features_val_transf @ text_features_pca.T)\n",
    "similarity = np.asarray(similarity).squeeze()#reshape((77535,15507 ))\n",
    "ut.calculate_recall(similarity, \"fpca_inf_flickr\")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c09b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "0       0.289       0.513        0.612"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4e239",
   "metadata": {},
   "source": [
    "# Clip-clip https://arxiv.org/abs/2109.05433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4d0a46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running MI G.T on the model ============== \n",
      "..... 400.........\n",
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female   Male  Disparity\n",
      "doctor      19.79  19.65      -0.14\n",
      "nurse       18.93  18.69      -0.24\n",
      "secretary   19.96  19.72      -0.24\n",
      "boss        20.59  20.40      -0.19\n",
      "lawyer      20.17  20.21       0.04\n",
      "paralegal   20.03  20.14       0.11\n",
      "-------------------------------------------------------------------\n",
      "       Query       stat          pval\n",
      "0     doctor  23.363667  1.340874e-06\n",
      "1      nurse  41.727508  1.049224e-10\n",
      "2  secretary  47.445306  5.656022e-12\n",
      "3       boss  34.601662  4.045606e-09\n",
      "4     lawyer   2.002213  1.570697e-01\n",
      "5  paralegal   9.738552  1.804430e-03\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             1.20             1.90             1.20\n",
      "1      nurse             0.18             0.30             0.34\n",
      "2  secretary             0.74             0.41             0.53\n",
      "3       boss             0.51             0.29             0.36\n",
      "4     lawyer             0.59             1.20             0.92\n",
      "5  paralegal             0.41             0.51             0.47\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.19        0.24        0.22\n",
      "1      nurse       -0.21       -0.21       -0.31\n",
      "2  secretary        0.29       -0.06        0.02\n",
      "3       boss       -0.51       -0.26       -0.11\n",
      "4     lawyer        0.19        0.34        0.29\n",
      "5  paralegal       -0.01        0.14        0.09\n",
      "..... 256.........\n",
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female   Male  Disparity\n",
      "doctor      19.53  19.34      -0.19\n",
      "nurse       18.73  18.45      -0.28\n",
      "secretary   19.60  19.44      -0.16\n",
      "boss        20.17  20.01      -0.16\n",
      "lawyer      19.98  19.93      -0.05\n",
      "paralegal   19.73  19.74       0.01\n",
      "-------------------------------------------------------------------\n",
      "       Query       stat          pval\n",
      "0     doctor  42.675171  6.462743e-11\n",
      "1      nurse  75.733738  3.246136e-18\n",
      "2  secretary  27.344993  1.702046e-07\n",
      "3       boss  31.363702  2.139435e-08\n",
      "4     lawyer   1.960373  1.614734e-01\n",
      "5  paralegal   0.049024  8.247709e-01\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             0.59             0.80             0.92\n",
      "1      nurse             0.74             0.67             0.34\n",
      "2  secretary             0.74             0.59             0.53\n",
      "3       boss             0.41             0.50             0.53\n",
      "4     lawyer             1.20             0.74             0.92\n",
      "5  paralegal             0.59             0.80             0.92\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.19        0.14        0.19\n",
      "1      nurse       -0.61       -0.56       -0.31\n",
      "2  secretary        0.19        0.09        0.09\n",
      "3       boss       -0.01        0.09        0.12\n",
      "4     lawyer        0.59        0.29        0.42\n",
      "5  paralegal        0.19        0.19        0.32\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running MI G.T on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    \n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features.cpu().numpy().astype(np.float64)\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_GT[attr]\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"..... {num_clip}.........\")\n",
    "        \n",
    "        text_features_mi =text_features[:, mis[:num_clip]]\n",
    "        image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "        similarity = (100.0 * image_features_val @ text_features_mi.T).T \n",
    "        ut.calc_similarity_diff(f'MI_gt{num_clip}_flickr','gender', queries, protected_attribute , {0: 'Female', 1:'Male'}, similarity)\n",
    "        ut.run_anova(queries, all_labels_gender_test, similarity, f'MI_gt{num_clip}_flickr', skip_att = 2)\n",
    "        ut.run_skew(queries, all_labels_gender_test, similarity, f'MI_gt{num_clip}_flickr', [10, 20, 30],skip_attr = 2)\n",
    "        ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, f'MI_gt{num_clip}_flickr', [10, 20, 30],skip_attr = 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "699b1a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running MI G.T recall on the model ============== \n",
      "   mean_top_1  mean_top_5  mean_top_10\n",
      "0       0.227       0.423        0.515\n",
      "1713.1582074165344\n",
      "   mean_top_1  mean_top_5  mean_top_10\n",
      "0       0.154        0.31        0.392\n",
      "1108.4078996181488\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running MI G.T recall on the model ============== \")\n",
    "\n",
    "flat_captions = all_labels_captions_test.flatten()\n",
    "for attr in ['gender']:\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_GT[attr]\n",
    "    for num_clip in num_clip_s:\n",
    "#         import time\n",
    "        start = time.time()\n",
    "        \n",
    "        similarity = []\n",
    "        \n",
    "        \n",
    "        image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "        \n",
    "        for cap in flat_captions:\n",
    "            text_inputs = clip.tokenize(f\"a photo of {cap}\", truncate = True).to(device)\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_inputs)#.float()\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features.cpu().numpy().astype(np.float64)\n",
    "            text_features_mi =text_features[:, mis[:num_clip]]\n",
    "            similarity.append((100.0 * image_features_val @ text_features_mi.T).T)\n",
    "        similarity = np.asarray(similarity).squeeze()#.reshape((77535,15507 ))\n",
    "        ut.calculate_recall(similarity, f\"MI_gt{num_clip}_flickr\")\n",
    "        end = time.time()\n",
    "        print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afb92199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running retrieval MI INF on the model ============== \n",
      "..... 400.........\n",
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female   Male  Disparity\n",
      "doctor      19.92  19.97       0.05\n",
      "nurse       18.83  18.78      -0.05\n",
      "secretary   19.81  19.74      -0.07\n",
      "boss        20.00  19.89      -0.11\n",
      "lawyer      20.09  20.30       0.21\n",
      "paralegal   19.83  20.07       0.24\n",
      "-------------------------------------------------------------------\n",
      "       Query       stat          pval\n",
      "0     doctor   2.356855  1.247335e-01\n",
      "1      nurse   2.057870  1.514217e-01\n",
      "2  secretary   2.868003  9.035691e-02\n",
      "3       boss  10.098162  1.484172e-03\n",
      "4     lawyer  35.662904  2.345888e-09\n",
      "5  paralegal  45.359308  1.640067e-11\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             1.20              1.2             0.69\n",
      "1      nurse             0.18              0.3             0.34\n",
      "2  secretary             0.41              0.5             0.47\n",
      "3       boss             1.20              0.8             0.92\n",
      "4     lawyer             1.20              1.9             2.30\n",
      "5  paralegal             1.20              0.8             0.51\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.19        0.19        0.05\n",
      "1      nurse       -0.11       -0.01       -0.01\n",
      "2  secretary       -0.11        0.04        0.09\n",
      "3       boss        0.09       -0.11       -0.01\n",
      "4     lawyer        0.19        0.34        0.29\n",
      "5  paralegal        0.39        0.09        0.09\n",
      "..... 256.........\n",
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female  Male  Disparity\n",
      "doctor       8.17  8.31       0.14\n",
      "nurse        8.32  8.48       0.16\n",
      "secretary    8.41  8.43       0.02\n",
      "boss         8.47  8.55       0.08\n",
      "lawyer       8.40  8.52       0.12\n",
      "paralegal    8.57  8.74       0.17\n",
      "-------------------------------------------------------------------\n",
      "       Query       stat          pval\n",
      "0     doctor  59.592943  1.166527e-14\n",
      "1      nurse  49.096247  2.437060e-12\n",
      "2  secretary   0.931056  3.345887e-01\n",
      "3       boss  17.610724  2.710555e-05\n",
      "4     lawyer  41.213644  1.364666e-10\n",
      "5  paralegal  64.852156  8.073371e-16\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             0.74             0.59             0.53\n",
      "1      nurse             0.51             0.05             0.34\n",
      "2  secretary             0.41             0.67             0.64\n",
      "3       boss             0.18             0.41             0.47\n",
      "4     lawyer             0.59             0.50             0.47\n",
      "5  paralegal             0.41             0.41             0.69\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.19        0.09        0.12\n",
      "1      nurse       -0.11       -0.21       -0.35\n",
      "2  secretary       -0.21        0.09        0.12\n",
      "3       boss       -0.31        0.04        0.05\n",
      "4     lawyer        0.19        0.09        0.05\n",
      "5  paralegal       -0.01       -0.01        0.26\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running retrieval MI INF on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    \n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in queries]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features.cpu().numpy().astype(np.float64)\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_inferred[attr]\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"..... {num_clip}.........\")\n",
    "        text_features_mi =text_features[:, mis[:num_clip]]\n",
    "        image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "        similarity = (100.0 * image_features_val @ text_features_mi.T).T \n",
    "        ut.calc_similarity_diff(f'MI_inf{num_clip}_flickr', 'gender',queries, protected_attribute , {0: 'Female', 1:'Male'}, similarity)\n",
    "        ut.run_anova(queries, all_labels_gender_test, similarity, f'MI_inf{num_clip}_flickr', skip_att = 2)\n",
    "        ut.run_skew(queries, all_labels_gender_test, similarity, f'MI_inf{num_clip}_flickr',[10, 20, 30],skip_attr = 2)\n",
    "        ut.run_retrieval_metric(queries, all_labels_gender_test, similarity, f'MI_inf{num_clip}_flickr',[10, 20, 30],skip_attr = 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87fec137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running MI INF recall on the model ============== \n",
      "   mean_top_1  mean_top_5  mean_top_10\n",
      "0       0.229       0.422        0.514\n",
      "1767.8580169677734\n",
      "   mean_top_1  mean_top_5  mean_top_10\n",
      "0       0.149       0.304        0.384\n",
      "1111.21799826622\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running MI INF recall on the model ============== \")\n",
    "\n",
    "flat_captions = all_labels_captions_test.flatten()\n",
    "for attr in ['gender']:\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_inferred[attr]\n",
    "    for num_clip in num_clip_s:\n",
    "#         import time\n",
    "        start = time.time()\n",
    "        similarity = []\n",
    "        image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "        for cap in flat_captions:\n",
    "            text_inputs = clip.tokenize(f\"a photo of {cap}\", truncate = True).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_inputs)#.float()\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features.cpu().numpy().astype(np.float64)\n",
    "            text_features_mi =text_features[:, mis[:num_clip]]\n",
    "            similarity.append((100.0 * image_features_val @ text_features_mi.T).T)\n",
    "        similarity = np.asarray(similarity).squeeze()#.reshape((77535,15507 ))\n",
    "        ut.calculate_recall(similarity, f\"MI_inf{num_clip}_flickr\")\n",
    "        end = time.time()\n",
    "        print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a88633",
   "metadata": {},
   "outputs": [],
   "source": [
    "======== Running MI INF recall on the model ============== \n",
    "   mean_top_1  mean_top_5  mean_top_10\n",
    "0       0.229       0.421        0.514\n",
    "2881.425798892975\n",
    "   mean_top_1  mean_top_5  mean_top_10\n",
    "0       0.149       0.304        0.384\n",
    "2773.1723685264587"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cf0ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running CLF MI G.T on the model ============== \n",
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "----------- 400--------------\n",
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.09  0.08      -0.01\n",
      "(nurse, doctor)                0.00  0.00       0.00\n",
      "(prostitute, night owl)        0.18  0.20       0.02\n",
      "(CEO, secretary)               0.99  0.99       0.00\n",
      "(leader, follower)             0.47  0.48       0.01\n",
      "(lawyer, paralegal)            0.88  0.85      -0.03\n",
      "(secretary, board member)      0.02  0.02       0.00\n",
      "(convict, exoneree)            0.89  0.86      -0.03\n",
      "----------- 256--------------\n",
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.04  0.04       0.00\n",
      "(nurse, doctor)                0.00  0.00       0.00\n",
      "(prostitute, night owl)        0.06  0.05      -0.01\n",
      "(CEO, secretary)               1.00  1.00       0.00\n",
      "(leader, follower)             0.44  0.48       0.04\n",
      "(lawyer, paralegal)            0.95  0.92      -0.03\n",
      "(secretary, board member)      0.01  0.01       0.00\n",
      "(convict, exoneree)            0.96  0.95      -0.01\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running CLF MI G.T on the model ============== \")\n",
    "\n",
    "for attr in ['gender']:\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_GT[attr]\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"----------- {num_clip}--------------\")\n",
    "        temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "        for cc, task in enumerate(classification_tasks[attr]):\n",
    "            text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_inputs)\n",
    "            text_features_mi =text_features.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            similarity = softmax(100.0 * np.matmul(image_features_val, np.transpose(text_features_mi)),axis=1)\n",
    "\n",
    "            predictions = np.argmax(similarity,axis=1)\n",
    "            for ell in range(2):\n",
    "    #             print(ell)\n",
    "                temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "        columns= ['Female', 'Male']\n",
    "        temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "        if attr == 'gender':\t  \n",
    "            temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "        elif attr == 'race':\n",
    "            temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "        temp.to_csv(f\"../results_csv/{attr}_flickr_clf_MI_gt{num_clip}.csv\")\n",
    "        print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767b3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08fe8dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running CLF MI inf on the model ============== \n",
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "----------- 400--------------\n",
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.11  0.10      -0.01\n",
      "(nurse, doctor)                0.00  0.00       0.00\n",
      "(prostitute, night owl)        0.22  0.26       0.04\n",
      "(CEO, secretary)               0.99  0.99       0.00\n",
      "(leader, follower)             0.37  0.38       0.01\n",
      "(lawyer, paralegal)            0.92  0.90      -0.02\n",
      "(secretary, board member)      0.02  0.02       0.00\n",
      "(convict, exoneree)            0.75  0.67      -0.08\n",
      "----------- 256--------------\n",
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.24  0.24       0.00\n",
      "(nurse, doctor)                0.17  0.17       0.00\n",
      "(prostitute, night owl)        0.36  0.42       0.06\n",
      "(CEO, secretary)               0.91  0.90      -0.01\n",
      "(leader, follower)             0.71  0.75       0.04\n",
      "(lawyer, paralegal)            0.58  0.56      -0.02\n",
      "(secretary, board member)      0.15  0.14      -0.01\n",
      "(convict, exoneree)            0.84  0.79      -0.05\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running CLF MI inf on the model ============== \")\n",
    "for attr in ['gender']:\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_inferred[attr]\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"----------- {num_clip}--------------\")\n",
    "        temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    \n",
    "        for cc, task in enumerate(classification_tasks[attr]):\n",
    "            text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in task]).to(device)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_inputs)\n",
    "            text_features_mi =text_features.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            image_features_val = all_features_test.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            similarity = softmax(100.0 * np.matmul(image_features_val, np.transpose(text_features_mi)),axis=1)\n",
    "\n",
    "            predictions = np.argmax(similarity,axis=1)\n",
    "            for ell in range(2):\n",
    "    #             print(ell)\n",
    "                temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test==ell]),2)\n",
    "        columns= ['Female', 'Male']\n",
    "        temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "        if attr == 'gender': \n",
    "            temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "        elif attr == 'race':\n",
    "            temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "        temp.to_csv(f\"../results_csv/{attr}_flickr_clf_MI_inf{num_clip}.csv\")\n",
    "        print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56b151",
   "metadata": {},
   "source": [
    "# Prompt method https://arxiv.org/abs/2203.11933"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "049aae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../debias-vision-lang')\n",
    "import debias_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abfa4c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing pretrained embedings\n",
      " best_ndkl_oai-clip-vit-b-16_neptune_run_OXVLB-317_model_e4_step_5334_embeddings.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4.73k/4.73k [00:00<00:00, 11.3MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [01:50<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "deb_clip_model, deb_preprocess = debias_clip.load(\"ViT-B/16-gender\", device=device)\n",
    "deb_clip_model.eval()\n",
    "flickrData = fl.MyFlickr30k('../../flicker30k-images/flickr30k-images', '../../flicker30k-captions/results_20130124.token',None, transform=deb_preprocess)\n",
    "train_size = int(0.5 * len(flickrData))\n",
    "test_size = len(flickrData) - train_size\n",
    "torch.manual_seed(0)\n",
    "set_seed(0)\n",
    "flickrData_train, flickrData_test = torch.utils.data.random_split(flickrData, [train_size, test_size])\n",
    "all_features_test_deb, all_labels_captions_test_deb, all_labels_gender_test_deb = ut.get_features_flickr(flickrData_test, deb_clip_model, device)\n",
    "all_features_test_deb /= all_features_test_deb.norm(dim=-1, keepdim=True)\n",
    "flat_captions = all_labels_captions_test_deb.flatten()\n",
    "protected_attribute = {'gender':all_labels_gender_test_deb}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "889e94e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing bias in debias model\n",
      "Installing pretrained embedings\n",
      " best_ndkl_oai-clip-vit-b-16_neptune_run_OXVLB-317_model_e4_step_5334_embeddings.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4.73k/4.73k [00:00<00:00, 10.6MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 77]) torch.Size([6, 512])\n",
      "--- Evaluation of mean similarity scores w.r.t. gender on Val ---\n",
      "gender ['Female', 'Male']\n",
      "           Female   Male  Disparity\n",
      "doctor      18.95  19.38       0.43\n",
      "nurse       19.91  18.44      -1.47\n",
      "secretary   20.61  19.86      -0.75\n",
      "boss        20.23  20.78       0.55\n",
      "lawyer      18.71  19.10       0.39\n",
      "paralegal   20.28  19.97      -0.31\n",
      "-------------------------------------------------------------------\n",
      "       Query         stat           pval\n",
      "0     doctor   221.919533   3.448994e-50\n",
      "1      nurse  1540.923601   0.000000e+00\n",
      "2  secretary   675.907256  5.187530e-149\n",
      "3       boss   487.376130  5.305659e-108\n",
      "4     lawyer   165.642349   6.622040e-38\n",
      "5  paralegal   185.275342   3.417344e-42\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             3.51             1.90             1.20\n",
      "1      nurse             1.20             1.90             1.61\n",
      "2  secretary             0.18             0.51             0.18\n",
      "3       boss             3.51             3.51             1.61\n",
      "4     lawyer             3.51             1.90             1.61\n",
      "5  paralegal             0.59             0.41             0.47\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor        0.29        0.19        0.22\n",
      "1      nurse       -0.81       -0.91       -0.78\n",
      "2  secretary       -0.31       -0.51       -0.28\n",
      "3       boss        0.49        0.59        0.49\n",
      "4     lawyer        0.69        0.64        0.62\n",
      "5  paralegal        0.09       -0.26       -0.08\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>ddp_top_10</th>\n",
       "      <th>ddp_top_20</th>\n",
       "      <th>ddp_top_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doctor</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nurse</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>-0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>secretary</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boss</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lawyer</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>paralegal</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
       "0     doctor        0.29        0.19        0.22\n",
       "1      nurse       -0.81       -0.91       -0.78\n",
       "2  secretary       -0.31       -0.51       -0.28\n",
       "3       boss        0.49        0.59        0.49\n",
       "4     lawyer        0.69        0.64        0.62\n",
       "5  paralegal        0.09       -0.26       -0.08"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Testing bias in debias model\")\n",
    "\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in queries]).to(\"cpu\")\n",
    "deb_clip_model_cpu, deb_preprocess = debias_clip.load(\"ViT-B/16-gender\", device='cpu')\n",
    "deb_clip_model_cpu.eval()\n",
    "with torch.no_grad():\n",
    "#     deb_clip_model = deb_clip_model.to(\"cpu\") # didn't work! \n",
    "    text_features_deb = deb_clip_model_cpu.encode_text(text_inputs).to(torch.float16)\n",
    "    text_features_deb = text_features_deb.to(device)\n",
    "print(text_inputs.shape, text_features_deb.shape)\n",
    "text_features_deb /= text_features_deb.norm(dim=-1, keepdim=True)\n",
    "similarity_deb = (100.0 * all_features_test_deb @ text_features_deb.T).cpu().numpy().astype(np.float64).T\n",
    "ut.calc_similarity_diff(f'prompt_flickr','gender', queries,  protected_attribute, {0: 'Female', 1:'Male'}, similarity_deb)\n",
    "ut.run_anova(queries, all_labels_gender_test_deb, similarity_deb, f'prompt_flickr', skip_att = 2)\n",
    "ut.run_skew(queries, all_labels_gender_test_deb, similarity_deb, f'prompt_flickr',[10, 20, 30],skip_attr = 2)\n",
    "ut.run_retrieval_metric(queries, all_labels_gender_test_deb, similarity_deb, f'prompt_flickr',[10, 20, 30],skip_attr = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f15c7307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running Prompt recall on the model ============== \n",
      "Installing pretrained embedings\n",
      " best_ndkl_oai-clip-vit-b-16_neptune_run_OXVLB-317_model_e4_step_5334_embeddings.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4.73k/4.73k [00:00<00:00, 10.2MiB/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 77535/77535 [20:47<00:00, 62.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_top_1  mean_top_5  mean_top_10\n",
      "0       0.353       0.593         0.69\n",
      "1327.7051665782928\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running Prompt recall on the model ============== \")\n",
    "import time\n",
    "for attr in ['gender']:\n",
    "    start = time.time()\n",
    "\n",
    "    similarity = []\n",
    "\n",
    "    deb_clip_model_cpu, deb_preprocess = debias_clip.load(\"ViT-B/16-gender\", device='cpu')\n",
    "    deb_clip_model_cpu.eval()\n",
    "    for cap in tqdm(flat_captions):\n",
    "        text_inputs = clip.tokenize(f\"a photo of {cap}\", truncate = True).to(\"cpu\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_features = deb_clip_model_cpu.encode_text(text_inputs).to(torch.float16)\n",
    "            text_features = text_features.to(device)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "      \n",
    "      \n",
    "        similarity.append((100.0 * all_features_test_deb @ text_features.T).cpu().numpy().astype(np.float64).T)\n",
    "    similarity = np.asarray(similarity).squeeze()\n",
    "    ut.calculate_recall(similarity, f\"prompt_flickr\")\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "754e92a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77535, 15507)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_top_1  mean_top_5  mean_top_10\n",
    "0       0.352       0.594         0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeafd4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.29        0.52         0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b78890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.353       0.594         0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b60d9e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "Installing pretrained embedings\n",
      " best_ndkl_oai-clip-vit-b-16_neptune_run_OXVLB-317_model_e4_step_5334_embeddings.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4.73k/4.73k [00:00<00:00, 9.42MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Female  Male  Disparity\n",
      "(criminal, innocent person)    0.64  0.83       0.19\n",
      "(nurse, doctor)                0.74  0.26      -0.48\n",
      "(prostitute, night owl)        0.67  0.49      -0.18\n",
      "(CEO, secretary)               0.19  0.57       0.38\n",
      "(leader, follower)             0.37  0.48       0.11\n",
      "(lawyer, paralegal)            0.07  0.23       0.16\n",
      "(secretary, board member)      0.46  0.24      -0.22\n",
      "(convict, exoneree)            0.20  0.11      -0.09\n"
     ]
    }
   ],
   "source": [
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    deb_clip_model_cpu, deb_preprocess = debias_clip.load(\"ViT-B/16-gender\", device='cpu')\n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in task])#.to(device)\n",
    "        with torch.no_grad():\n",
    "#     deb_clip_model = deb_clip_model.to(\"cpu\") # didn't work! \n",
    "            text_features_deb = deb_clip_model_cpu.encode_text(text_inputs).to(torch.float16)\n",
    "            text_features_deb = text_features_deb.to(device)\n",
    "        text_features_deb /= text_features_deb.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * all_features_test_deb @ text_features_deb.T).softmax(dim=-1).cpu().numpy().astype(np.float64)\n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        for ell in range(2):\n",
    "#             print(ell)\n",
    "            temp[cc, ell] = 1 - np.around(np.mean(predictions[all_labels_gender_test_deb==ell]),2)\n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = temp['Male'] - temp['Female']\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_flickr_clf_prompt.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf0397d",
   "metadata": {},
   "source": [
    "# Explicit gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c278bcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Run gendered ------------------\n",
      "       Query  abs_skew_top_10  abs_skew_top_20  abs_skew_top_30\n",
      "0     doctor             0.51             0.29             0.10\n",
      "1      nurse             0.41             0.30             0.41\n",
      "2  secretary             0.41             0.41             0.34\n",
      "3       boss             0.41             0.30             0.26\n",
      "4     lawyer             0.51             0.30             0.26\n",
      "5  paralegal             0.41             0.30             0.26\n",
      "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
      "0     doctor       -0.31       -0.36       -0.25\n",
      "1      nurse       -0.41       -0.26       -0.35\n",
      "2  secretary       -0.11       -0.16       -0.25\n",
      "3       boss       -0.01       -0.11       -0.18\n",
      "4     lawyer       -0.01       -0.06       -0.15\n",
      "5  paralegal       -0.21       -0.16       -0.21\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>ddp_top_10</th>\n",
       "      <th>ddp_top_20</th>\n",
       "      <th>ddp_top_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doctor</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nurse</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>secretary</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boss</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lawyer</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>paralegal</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Query  ddp_top_10  ddp_top_20  ddp_top_30\n",
       "0     doctor       -0.31       -0.36       -0.25\n",
       "1      nurse       -0.41       -0.26       -0.35\n",
       "2  secretary       -0.11       -0.16       -0.25\n",
       "3       boss       -0.01       -0.11       -0.18\n",
       "4     lawyer       -0.01       -0.06       -0.15\n",
       "5  paralegal       -0.21       -0.16       -0.21"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"----------------- Run gendered ------------------\")\n",
    "word_list_gendered = []\n",
    "for word in queries:\n",
    "    word_list_gendered.append(f'male {word}')\n",
    "    word_list_gendered.append(f'female {word}')\n",
    "      \n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {word}\") for word in word_list_gendered]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity_gendered = (100.0 * all_features_test @ text_features.T).cpu().numpy().astype(np.float64).T\n",
    "ut.run_skew_mixed(queries, similarity_gendered, all_labels_gender_test, 'gen_bln_flickr', [10,20,30], skip_attr = 2)\n",
    "ut.run_retrieval_metric_mixed(queries, similarity_gendered, all_labels_gender_test, 'gen_bln_flickr', [10,20,30], skip_attr = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "004dda2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(all_labels_gender_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "659d53c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "len(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "992a246a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ...,  0, -1, -2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels_gender_test - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa25d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
