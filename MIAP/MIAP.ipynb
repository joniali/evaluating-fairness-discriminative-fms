{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722971d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "from PIL import Image\n",
    "import os\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from fair_pca import FairPCA,FairKernelPCA\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import utils as ut\n",
    "plt.ion()\n",
    "\n",
    "import miap_dataset as md\n",
    "import importlib\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23002815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'miap_dataset' from '/mnt/efs/fairclip/FinalCode/MIAP/miap_dataset.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(ut)\n",
    "importlib.reload(md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3d397b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:2\" #if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d81465",
   "metadata": {},
   "outputs": [],
   "source": [
    "miap_dataset_orig = md.MIAPDataset('../../miap','test' )\n",
    "miap_dataset_trans = md.MIAPDataset('../../miap', 'test',transform = preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67feb329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22590"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(miap_dataset_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cda6b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataset, model):\n",
    "    all_features = []\n",
    "    all_gender = []\n",
    "    all_age = []\n",
    "    all_area = []\n",
    "    all_num_people = []\n",
    "    with torch.no_grad():\n",
    "        for images, gen, age, area, num_people in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            \n",
    "            features = model.encode_image(images.to(device))\n",
    "            all_features.append(features)\n",
    "            all_gender.append(gen)\n",
    "            all_age.append(age)\n",
    "            all_area.append(area)\n",
    "            all_num_people.append(num_people)\n",
    "    return torch.cat(all_features), torch.cat(all_gender).cpu().numpy(), torch.cat(all_area).cpu().numpy(), torch.cat(all_age).cpu().numpy(), torch.cat(all_num_people).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f7e8f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 226/226 [56:40<00:00, 15.05s/it]\n"
     ]
    }
   ],
   "source": [
    "features, gender, area, age, no_people =  get_features(miap_dataset_trans, model)\n",
    "features /= features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb26622",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_tasks = {}\n",
    "classification_tasks['gender'] = [\n",
    "#     ('a photo of a woman','a photo of a man'),\n",
    "    ('an inconspicuous photo of a person','a prominent photo of a person'),\n",
    "    ('a photo of a child', 'a photo of an adult'),\n",
    "    ('a photo of one person', 'a photo of more than one person'),\n",
    "    \n",
    "]\n",
    "gt_labels = {0:area,1:age,2:no_people }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae19fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_eop(text_features, image_features,gt_labels, attr, method, gender):\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1).cpu().numpy().astype(np.float64)\n",
    "    predictions = np.argmax(similarity,axis=1)\n",
    "    idx_ = np.where(np.logical_and(gender != 2, age != 2))\n",
    "    accs.append(np.around(np.mean(predictions[idx_] == gt_labels[cc][idx_]),2))\n",
    "    print(task,np.around(np.mean(predictions[idx_] == gt_labels[cc][idx_]),2), np.mean(predictions[idx_]) )\n",
    "\n",
    "    for ell in range(2):\n",
    "        idx_ = np.where(np.logical_and(gender == ell, age != 2))            \n",
    "        idx_tp = np.where(np.logical_and(gt_labels[cc][idx_] == 1, predictions[idx_] == 1))[0]\n",
    "\n",
    "        pos_ = np.where(gt_labels[cc][idx_] == 1)[0].shape[0]\n",
    "        temp[cc, ell] = idx_tp.shape[0]/ pos_\n",
    "            \n",
    "\n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = abs(temp['Male'] - temp['Female'])\n",
    "        temp['Accuracy'] = accs\n",
    "    temp.to_csv(f\"../results_csv/{attr}_miap_clf_{method}.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a664168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "('an inconspicuous photo of a person', 'a prominent photo of a person') 0.62 0.8427157001414427\n",
      "('a photo of a child', 'a photo of an adult') 0.92 0.9181046676096181\n",
      "('a photo of one person', 'a photo of more than one person') 0.66 0.2780763790664781\n",
      "                                                      Female      Male  \\\n",
      "(an inconspicuous photo of a person, a prominen...  0.925672  0.923591   \n",
      "(a photo of a child, a photo of an adult)           0.867606  0.945474   \n",
      "(a photo of one person, a photo of more than on...  0.218419  0.652610   \n",
      "\n",
      "                                                    Disparity  Accuracy  \n",
      "(an inconspicuous photo of a person, a prominen...   0.002081      0.62  \n",
      "(a photo of a child, a photo of an adult)            0.077869      0.92  \n",
      "(a photo of one person, a photo of more than on...   0.434191      0.66  \n"
     ]
    }
   ],
   "source": [
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    accs = []\n",
    "    tp_idxs = []\n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([clip.tokenize(word) for word in task]).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * features @ text_features.T).softmax(dim=-1).cpu().numpy().astype(np.float64)\n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        idx_ = np.where(np.logical_and(gender != 2, age != 2))\n",
    "        accs.append(np.around(np.mean(predictions[idx_] == gt_labels[cc][idx_]),2))\n",
    "        print(task,np.around(np.mean(predictions[idx_] == gt_labels[cc][idx_]),2), np.mean(predictions[idx_]) )\n",
    "#         if cc == 0:\n",
    "#             continue\n",
    "        \n",
    "        for ell in range(2):\n",
    "#             print(ell)\n",
    "            idx_ = np.where(np.logical_and(gender == ell, age != 2))\n",
    "#             print(idx_[0].shape)\n",
    "#             temp[cc, ell] = np.around(np.mean(predictions[id_] == data['labels'][gt_label[cc]][idx_]),2)\n",
    "            \n",
    "            \n",
    "            idx_tp = np.where(np.logical_and(gt_labels[cc][idx_] == 1, predictions[idx_] == 1))[0]\n",
    "            \n",
    "            pos_ = np.where(gt_labels[cc][idx_] == 1)[0].shape[0]\n",
    "            temp[cc, ell] = idx_tp.shape[0]/ pos_\n",
    "            \n",
    "#             temp[cc, ell] = 1 - np.around(np.mean(predictions[data['labels']['gender']== ell] != data['labels'][]),2)\n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = abs(temp['Male'] - temp['Female'])\n",
    "        temp['Accuracy'] = accs\n",
    "    temp.to_csv(f\"../results_csv/{attr}_miap_clf_orig.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10ec1480",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (368994024.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    --- Evaluation of zero-shot classification w.r.t. gender  -------------------------\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
    "# Numbers are the mean prediction rate for the first word when classifying into the two words\n",
    "# ('an inconspicuous photo of a person', 'a prominent photo of a person') 0.62 0.8407355021216407\n",
    "# ('a photo of a child', 'a photo of an adult') 0.92 0.9183875530410184\n",
    "# ('a photo of one person', 'a photo of more than one person') 0.66 0.2775106082036775\n",
    "#                                                       Female      Male  \\\n",
    "# (an inconspicuous photo of a person, a prominen...  0.921670  0.922159   \n",
    "# (a photo of a child, a photo of an adult)           0.867606  0.945911   \n",
    "# (a photo of one person, a photo of more than on...  0.216553  0.653949   \n",
    "\n",
    "#                                                     Disparity  Accuracy  \n",
    "# (an inconspicuous photo of a person, a prominen...   0.000489      0.62  \n",
    "# (a photo of a child, a photo of an adult)            0.078305      0.92  \n",
    "# (a photo of one person, a photo of more than on...   0.437397      0.66 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "697b639e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 700/700 [3:29:43<00:00, 17.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error of predicting gender train = 0.05\n"
     ]
    }
   ],
   "source": [
    "projection_GT,projection_inferred, MI_GT, MI_inferred = ut.calculate_projections_miap(model, preprocess, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5357a87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "('an inconspicuous photo of a person', 'a prominent photo of a person') 0.6 0.2079207920792079\n",
      "('a photo of a child', 'a photo of an adult') 0.97 0.9680339462517681\n",
      "('a photo of one person', 'a photo of more than one person') 0.62 0.3968882602545969\n",
      "                                                      Female      Male  \\\n",
      "(an inconspicuous photo of a person, a prominen...  0.300743  0.334288   \n",
      "(a photo of a child, a photo of an adult)           0.956539  0.974264   \n",
      "(a photo of one person, a photo of more than on...  0.387679  0.655957   \n",
      "\n",
      "                                                    Disparity  Accuracy  \n",
      "(an inconspicuous photo of a person, a prominen...   0.033545      0.60  \n",
      "(a photo of a child, a photo of an adult)            0.017725      0.97  \n",
      "(a photo of one person, a photo of more than on...   0.268278      0.62  \n"
     ]
    }
   ],
   "source": [
    "# FPCA GT\n",
    "from scipy.special import softmax\n",
    "\n",
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    accs = []\n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a person {word}.\") for word in task]).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        projection_train = projection_GT[attr]\n",
    "        idx_ = np.where(np.logical_and(gender != 2, age != 2))\n",
    "        all_features_val_transf = projection_train.just_transform(features.cpu().numpy().astype(np.float64))\n",
    "        text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "        similarity = softmax(100.0 * np.matmul(all_features_val_transf, np.transpose(text_features_pca)),axis=1)\n",
    "        \n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        print(task,np.around(np.mean(predictions[idx_] == gt_labels[cc][idx_]),2), np.mean(predictions[idx_]))\n",
    "        accs.append(np.around(np.mean(predictions[idx_] == gt_labels[cc][idx_]),2))\n",
    "#         if cc == 0:\n",
    "#             continue\n",
    "        for ell in range(2):\n",
    "            idx_ = np.where(np.logical_and(gender == ell, age != 2))\n",
    "            \n",
    "#             temp[cc, ell] = np.around(np.mean(predictions[idx_] == data['labels'][gt_label[cc]][idx_]),2)\n",
    "            \n",
    "            \n",
    "            idx_tp = np.where(np.logical_and(gt_labels[cc][idx_] == 1, predictions[idx_] == 1))[0]\n",
    "            \n",
    "            pos_ = np.where(gt_labels[cc][idx_] == 1)[0].shape[0]\n",
    "            temp[cc, ell] = idx_tp.shape[0]/ pos_\n",
    "            \n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = abs(temp['Male'] - temp['Female'])\n",
    "        temp['Accuracy'] = accs\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_miap_clf_fpca_gt.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df9a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers are the mean prediction rate for the first word when classifying into the two words\n",
    "# ('an inconspicuous photo of a person', 'a prominent photo of a person') 0.59 0.20862800565770862\n",
    "# ('a photo of a child', 'a photo of an adult') 0.97 0.9677510608203678\n",
    "# ('a photo of one person', 'a photo of more than one person') 0.62 0.3983026874115983\n",
    "#                                                       Female      Male  \\\n",
    "# (an inconspicuous photo of a person, a prominen...  0.301887  0.333333   \n",
    "# (a photo of a child, a photo of an adult)           0.956942  0.973610   \n",
    "# (a photo of one person, a photo of more than on...  0.389546  0.657965   \n",
    "\n",
    "#                                                     Disparity  Accuracy  \n",
    "# (an inconspicuous photo of a person, a prominen...   0.031447      0.59  \n",
    "# (a photo of a child, a photo of an adult)            0.016668      0.97  \n",
    "# (a photo of one person, a photo of more than on...   0.268419      0.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5315d7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "('an inconspicuous photo of a person', 'a prominent photo of a person') 0.6 0.20947666195190948\n",
      "('a photo of a child', 'a photo of an adult') 0.97 0.9694483734087694\n",
      "('a photo of one person', 'a photo of more than one person') 0.61 0.3908062234794908\n",
      "                                                      Female      Male  \\\n",
      "(an inconspicuous photo of a person, a prominen...  0.308748  0.333811   \n",
      "(a photo of a child, a photo of an adult)           0.960563  0.974264   \n",
      "(a photo of one person, a photo of more than on...  0.359676  0.664659   \n",
      "\n",
      "                                                    Disparity  Accuracy  \n",
      "(an inconspicuous photo of a person, a prominen...   0.025063      0.60  \n",
      "(a photo of a child, a photo of an adult)            0.013701      0.97  \n",
      "(a photo of one person, a photo of more than on...   0.304982      0.61  \n"
     ]
    }
   ],
   "source": [
    "# FPCA inf\n",
    "from scipy.special import softmax\n",
    "\n",
    "for attr in ['gender']:\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "    accs = []\n",
    "    for cc, task in enumerate(classification_tasks[attr]):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a person {word}.\") for word in task]).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        projection_train = projection_inferred[attr]\n",
    "        idx_ = np.where(np.logical_and(gender != 2, age != 2))\n",
    "        all_features_val_transf = projection_train.just_transform(features.cpu().numpy().astype(np.float64))\n",
    "        text_features_pca = projection_train.just_transform(text_features.cpu().numpy().astype(np.float64))\n",
    "        similarity = softmax(100.0 * np.matmul(all_features_val_transf, np.transpose(text_features_pca)),axis=1)\n",
    "        \n",
    "        predictions = np.argmax(similarity,axis=1)\n",
    "        print(task,np.around(np.mean(predictions[idx_] == gt_labels[cc][idx_]),2), np.mean(predictions[idx_]))\n",
    "        accs.append(np.around(np.mean(predictions[idx_] == gt_labels[cc][idx_]),2))\n",
    "#         if cc == 0:\n",
    "#             continue\n",
    "        for ell in range(2):\n",
    "            idx_ = np.where(np.logical_and(gender == ell, age != 2))\n",
    "           \n",
    "            \n",
    "            idx_tp = np.where(np.logical_and(gt_labels[cc][idx_] == 1, predictions[idx_] == 1))[0]\n",
    "            \n",
    "            pos_ = np.where(gt_labels[cc][idx_] == 1)[0].shape[0]\n",
    "#             print(\"pos: \", pos_)\n",
    "            temp[cc, ell] = idx_tp.shape[0]/ pos_\n",
    "            \n",
    "    columns= ['Female', 'Male']\n",
    "    temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "    if attr == 'gender':\t  \n",
    "        temp['Disparity'] = abs(temp['Male'] - temp['Female'])\n",
    "        temp['Accuracy'] = accs\n",
    "    elif attr == 'race':\n",
    "        temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "    temp.to_csv(f\"../results_csv/{attr}_miap_clf_fpca_inf.csv\")\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6a6162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running CLF MI G.T on the model ============== \n",
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "----------- 400--------------\n",
      "('an inconspicuous photo of a person', 'a prominent photo of a person') 0.6 0.7015558698727016\n",
      "('a photo of a child', 'a photo of an adult') 0.66 0.6598302687411598\n",
      "('a photo of one person', 'a photo of more than one person') 0.54 0.3760961810466761\n",
      "                                                      Female      Male  \\\n",
      "(an inconspicuous photo of a person, a prominen...  0.762150  0.785100   \n",
      "(a photo of a child, a photo of an adult)           0.678873  0.649509   \n",
      "(a photo of one person, a photo of more than on...  0.357187  0.459839   \n",
      "\n",
      "                                                    Disparity  Accuracy  \n",
      "(an inconspicuous photo of a person, a prominen...   0.022950      0.60  \n",
      "(a photo of a child, a photo of an adult)            0.029364      0.66  \n",
      "(a photo of one person, a photo of more than on...   0.102652      0.54  \n",
      "0.05165550287292444\n",
      "----------- 256--------------\n",
      "('an inconspicuous photo of a person', 'a prominent photo of a person') 0.61 0.5326732673267327\n",
      "('a photo of a child', 'a photo of an adult') 0.56 0.5551626591230552\n",
      "('a photo of one person', 'a photo of more than one person') 0.54 0.4748231966053748\n",
      "                                                      Female      Male  \\\n",
      "(an inconspicuous photo of a person, a prominen...  0.638651  0.622732   \n",
      "(a photo of a child, a photo of an adult)           0.553320  0.556161   \n",
      "(a photo of one person, a photo of more than on...  0.527069  0.502008   \n",
      "\n",
      "                                                    Disparity  Accuracy  \n",
      "(an inconspicuous photo of a person, a prominen...   0.015919      0.61  \n",
      "(a photo of a child, a photo of an adult)            0.002841      0.56  \n",
      "(a photo of one person, a photo of more than on...   0.025061      0.54  \n",
      "0.014607186799954616\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running CLF MI G.T on the model ============== \")\n",
    "\n",
    "for attr in ['gender']:\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_GT[attr]\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"----------- {num_clip}--------------\")\n",
    "        temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "        accs = []\n",
    "        for cc, task in enumerate(classification_tasks[attr]):\n",
    "            text_inputs = torch.cat([clip.tokenize(f\"a photo of a person {word}.\") for word in task]).to(device)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_inputs)\n",
    "            idx_ = np.where(np.logical_and(gender != 2, age != 2))\n",
    "            text_features_mi =text_features.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            image_features_val = features.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            similarity = softmax(100.0 * np.matmul(image_features_val, np.transpose(text_features_mi)),axis=1)\n",
    "\n",
    "            predictions = np.argmax(similarity,axis=1)\n",
    "            print(task,np.around(np.mean(predictions[idx_] == gt_labels[cc][idx_]),2), np.mean(predictions[idx_]))\n",
    "            accs.append(np.around(np.mean(predictions[idx_] == gt_labels[cc][idx_]),2))\n",
    "#             if cc == 0:\n",
    "#                 continue\n",
    "            for ell in range(2):\n",
    "                idx_ = np.where(np.logical_and(gender == ell, age != 2))\n",
    "           \n",
    "            \n",
    "                idx_tp = np.where(np.logical_and(gt_labels[cc][idx_] == 1, predictions[idx_] == 1))[0]\n",
    "\n",
    "                pos_ = np.where(gt_labels[cc][idx_] == 1)[0].shape[0]\n",
    "#                 print(idx_[0].shape, pos_)\n",
    "                temp[cc, ell] = idx_tp.shape[0]/ pos_\n",
    "                \n",
    "        columns= ['Female', 'Male']\n",
    "        temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "        if attr == 'gender':\t  \n",
    "            temp['Disparity'] = abs(temp['Male'] - temp['Female'])\n",
    "            temp['Accuracy'] = accs\n",
    "        elif attr == 'race':\n",
    "            temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "        temp.to_csv(f\"../results_csv/{attr}_miap_clf_MI_gt{num_clip}.csv\")\n",
    "        print(temp)\n",
    "        print(np.mean(temp.Disparity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ba57f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running CLF MI inferred on the model ============== \n",
      "--- Evaluation of zero-shot classification w.r.t. gender  -------------------------\n",
      "Numbers are the mean prediction rate for the first word when classifying into the two words\n",
      "----------- 400--------------\n",
      "('an inconspicuous photo of a person', 'a prominent photo of a person') 0.63 0.660961810466761\n",
      "('a photo of a child', 'a photo of an adult') 0.67 0.6684582743988685\n",
      "('a photo of one person', 'a photo of more than one person') 0.55 0.27934936350777934\n",
      "                                                      Female      Male  \\\n",
      "(an inconspicuous photo of a person, a prominen...  0.759291  0.768386   \n",
      "(a photo of a child, a photo of an adult)           0.681288  0.661505   \n",
      "(a photo of one person, a photo of more than on...  0.215308  0.394913   \n",
      "\n",
      "                                                    Disparity  Accuracy  \n",
      "(an inconspicuous photo of a person, a prominen...   0.009095      0.63  \n",
      "(a photo of a child, a photo of an adult)            0.019783      0.67  \n",
      "(a photo of one person, a photo of more than on...   0.179605      0.55  \n",
      "0.06949420595945685\n",
      "----------- 256--------------\n",
      "('an inconspicuous photo of a person', 'a prominent photo of a person') 0.63 0.5606789250353607\n",
      "('a photo of a child', 'a photo of an adult') 0.63 0.6316831683168317\n",
      "('a photo of one person', 'a photo of more than one person') 0.53 0.4504950495049505\n",
      "                                                      Female      Male  \\\n",
      "(an inconspicuous photo of a person, a prominen...  0.675243  0.678606   \n",
      "(a photo of a child, a photo of an adult)           0.657143  0.617884   \n",
      "(a photo of one person, a photo of more than on...  0.495955  0.469880   \n",
      "\n",
      "                                                    Disparity  Accuracy  \n",
      "(an inconspicuous photo of a person, a prominen...   0.003363      0.63  \n",
      "(a photo of a child, a photo of an adult)            0.039258      0.63  \n",
      "(a photo of one person, a photo of more than on...   0.026076      0.53  \n",
      "0.022898891018890626\n"
     ]
    }
   ],
   "source": [
    "print(\"======== Running CLF MI inferred on the model ============== \")\n",
    "\n",
    "for attr in ['gender']:\n",
    "    num_clip_s = [400, 256]\n",
    "    mis = MI_inferred[attr]\n",
    "    print(f'--- Evaluation of zero-shot classification w.r.t. {attr}  -------------------------')\n",
    "    print('Numbers are the mean prediction rate for the first word when classifying into the two words')\n",
    "    for num_clip in num_clip_s:\n",
    "        print(f\"----------- {num_clip}--------------\")\n",
    "        temp = np.zeros((len(classification_tasks[attr]),2))\n",
    "        accs = []\n",
    "        for cc, task in enumerate(classification_tasks[attr]):\n",
    "            text_inputs = torch.cat([clip.tokenize(f\"a photo of a person {word}.\") for word in task]).to(device)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_inputs)\n",
    "            text_features_mi =text_features.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            image_features_val = features.cpu().numpy().astype(np.float64)[:, mis[:num_clip]]\n",
    "            similarity = softmax(100.0 * np.matmul(image_features_val, np.transpose(text_features_mi)),axis=1)\n",
    "            idx_ = np.where(np.logical_and(gender != 2, age != 2))\n",
    "            predictions = np.argmax(similarity,axis=1)\n",
    "            print(task,np.around(np.mean(predictions[idx_] == gt_labels[cc][idx_]),2), np.mean(predictions[idx_]))\n",
    "            accs.append(np.around(np.mean(predictions[idx_] == gt_labels[cc][idx_]),2))\n",
    "#             if cc == 0:\n",
    "#                 continue\n",
    "            for ell in range(2):\n",
    "                idx_ = np.where(np.logical_and(gender == ell, age != 2))\n",
    "           \n",
    "            \n",
    "                idx_tp = np.where(np.logical_and(gt_labels[cc][idx_] == 1, predictions[idx_] == 1))[0]\n",
    "\n",
    "                pos_ = np.where(gt_labels[cc][idx_] == 1)[0].shape[0]\n",
    "#                 print(idx_[0].shape, pos_)\n",
    "                temp[cc, ell] = idx_tp.shape[0]/ pos_\n",
    "                \n",
    "        columns= ['Female', 'Male']\n",
    "        temp = pd.DataFrame(temp, columns=columns, index=classification_tasks[attr])\n",
    "        if attr == 'gender':\t  \n",
    "            temp['Disparity'] = abs(temp['Male'] - temp['Female'])\n",
    "            temp['Accuracy'] = accs\n",
    "        elif attr == 'race':\n",
    "            temp['Disparity'] = temp.max(axis = 1) - temp.min(axis = 1)\n",
    "        temp.to_csv(f\"../results_csv/{attr}_miap_clf_MI_inf{num_clip}.csv\")\n",
    "        print(temp)\n",
    "        print(np.mean(temp.Disparity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727fc918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
